{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b714c30-b71c-43c6-9fe3-9b012a7bfdfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  MODIS Water Cluster Training\n",
    "\n",
    "Version: 0.1.0\n",
    "\n",
    "Date modified: 05.01.2023\n",
    "\n",
    "Modified by: Amanda Burke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ef8ad40-b2c6-4ccc-9a4b-c595bcce20e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path   \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier as skRF\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, precision_score, f1_score\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, matthews_corrcoef\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold, StratifiedKFold\n",
    "#from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "# #GDAL Stuff\n",
    "# from osgeo import gdalconst\n",
    "# from osgeo import gdal\n",
    "# from pprint import pprint\n",
    "\n",
    "# GPU-based frameworks\n",
    "\n",
    "import cudf\n",
    "import cupy as cp\n",
    "from cuml.ensemble import RandomForestClassifier as cuRFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef09a8be-17ed-40a8-958e-ed66b83929bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = False\n",
    "MODEL = 'rf'\n",
    "TEST_RATIO = 0.2\n",
    "RANDOM_STATE = 42\n",
    "LABEL_NAME = 'water'\n",
    "DATA_TYPE = np.int16\n",
    "FRAC_LAND=0.5\n",
    "num_datapoints = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb0fe47-657c-4bdd-b41d-cb8e70068281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/explore/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/training_data/v2.0.1/MOD09_GLOBAL_5469777_2_0_1.parquet.gzip']\n",
      "/explore/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/training_data/v2.0.1/MOD09_GLOBAL_5469777_2_0_1.parquet.gzip\n"
     ]
    }
   ],
   "source": [
    "# #############################\n",
    "# # VERSION 4.2.1 (targeted 500k points)\n",
    "# TILE_IN = 'Golden'#v4.2.1\n",
    "# DATA_VERSION='v4.2.1'\n",
    "# offsets_indexes = ['x_offset', 'y_offset', 'year', 'julian_day','tileID']\n",
    "# #############################\n",
    "\n",
    "##############################\n",
    "#VERSION 2.0.1 (5 million points)\n",
    "TILE_IN = 'GLOBAL'#v2.0.1\n",
    "DATA_VERSION='v2.0.1'\n",
    "offsets_indexes = ['x_offset', 'y_offset', 'year', 'julian_day']\n",
    "##############################\n",
    "\n",
    "# #############################\n",
    "# #VERSION 0.0.0 (2billion data points)\n",
    "# TILE_IN = 'cleaned'#v2.0.1\n",
    "# DATA_VERSION='AGU'\n",
    "# offsets_indexes = []#'x_offset', 'y_offset', 'year', 'julian_day']\n",
    "# ##############################\n",
    "\n",
    "training_data_basepath = f'/explore/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/training_data/{DATA_VERSION}'\n",
    "glob_string = os.path.join(training_data_basepath,'MOD*{}*.parquet.gzip'.format(TILE_IN))\n",
    "data_paths = sorted([fv for fv in glob.glob(glob_string)])\n",
    "\n",
    "#Only want the one with 4.2.0 because the other file doesnt work. \n",
    "print(data_paths)\n",
    "data_path = data_paths[0]\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fd27567-2a51-4e94-8d0a-43504a90bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cpu_data(fpath, colsToDrop, yCol='water', testSize=0.2, randomState=42, \n",
    "              dataType=np.float32, cpu=True, splitXY=False, trainTestSplit=False,\n",
    "             applyLog=False, imbalance=False, frac=0.1, land=False, multi=False, \n",
    "              multisample=1000000):\n",
    "    \"\"\"\n",
    "    Simple helper function for loading data to be used by models\n",
    "    :param fpath: Path to the data to be ingested.\n",
    "    :param dataType: Data type to convert ingested data to.\n",
    "    :param colsToDrop: Columns which are not necessary, from which to drop.\n",
    "    :param testSize: Ration to\n",
    "    \"\"\"\n",
    "    if multi:\n",
    "        all_dfs = [pd.read_csv(path_) for path_ in fpath]\n",
    "        df = pd.concat(all_dfs).sample(n=multisample, random_state=randomState)\n",
    "        print('DF length: {}'.format(len(df.index)))\n",
    "    else:   \n",
    "        df = pd.read_parquet(fpath) if '.parquet' in fpath else pd.read_csv(fpath)\n",
    "    df = df[df['sur_refl_b01_1'] + df['sur_refl_b02_1'] != 0]\n",
    "    df = df[df['sur_refl_b07_1'] + df['sur_refl_b02_1'] != 0]\n",
    "    df = df[df['sur_refl_b06_1'] + df['sur_refl_b02_1'] != 0]\n",
    "\n",
    "    df = df.drop(columns=colsToDrop)\n",
    "    cleanedDF = df[~df.isin([np.NaN, np.inf, -np.inf]).any(1)].dropna(axis=0).astype(dataType)\n",
    "    if applyLog:\n",
    "        for col in cleanedDF.drop([yCol], axis=1).columns:\n",
    "            print('Applying log1p func to {}'.format(col))\n",
    "            cleanedDF[col] = np.log1p(cleanedDF[col])\n",
    "        cleanedDF = cleanedDF[~cleanedDF.isin([np.NaN, np.inf, -np.inf]).any(1)].dropna(axis=0)\n",
    "    df = None\n",
    "    if imbalance:\n",
    "        if land:\n",
    "            print('Imbalancing data, sampling {} from water'.format(frac))\n",
    "        else:\n",
    "            print(f'Imbalancing data, sampling {frac} from land, {1-frac} from water')\n",
    "        groupedDF = cleanedDF.groupby('water')\n",
    "        dfs = [groupedDF.get_group(y) for y in groupedDF.groups]\n",
    "        sampledDF = dfs[1].sample(frac=frac)if land else dfs[0].sample(frac=frac)\n",
    "        concatDF = sampledDF.append(dfs[0]) if land else sampledDF.append(dfs[1])\n",
    "        concatDF = concatDF.sample(frac=1)\n",
    "        concatDF = concatDF.reset_index()\n",
    "        cleanedDF = concatDF.drop(columns=['index'])\n",
    "    if not splitXY:\n",
    "        return cleanedDF\n",
    "    X = cleanedDF.drop([yCol], axis=1).astype(dataType)\n",
    "    y = cleanedDF[yCol].astype(dataType)\n",
    "    if trainTestSplit:\n",
    "        return train_test_split(X, y, test_size=TEST_RATIO)\n",
    "    else:\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdee2440-7931-4da3-80e7-f79aefcb15dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "colsToDrop = [\n",
    "            #'sur_refl_b01_1',\n",
    "            # 'sur_refl_b02_1',\n",
    "             'sur_refl_b03_1',\n",
    "             'sur_refl_b04_1','sur_refl_b05_1','sur_refl_b06_1',\n",
    "            # 'sur_refl_b07_1',\n",
    "             # 'ndvi',\n",
    "             'ndwi1','ndwi2'\n",
    "            ]\n",
    "\n",
    "colsToDropTraining = colsToDrop.copy()\n",
    "colsToDropTraining.extend(offsets_indexes)\n",
    "v_names = ['sur_refl_b01_1','sur_refl_b02_1','sur_refl_b03_1',\n",
    "           'sur_refl_b04_1','sur_refl_b05_1','sur_refl_b06_1',\n",
    "           'sur_refl_b07_1','ndvi','ndwi1','ndwi2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133868b4-66bb-4f85-bcb2-641a79f06e2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b8a3852-14ee-4615-bc7c-871046ca19e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sur_refl_b03_1',\n",
       " 'sur_refl_b04_1',\n",
       " 'sur_refl_b05_1',\n",
       " 'sur_refl_b06_1',\n",
       " 'ndwi1',\n",
       " 'ndwi2']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colsToDrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb281755-23fe-4789-ba8b-584105111691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (4375821, 4), (4375821,)\n",
      "CPU times: user 3.9 s, sys: 1.05 s, total: 4.95 s\n",
      "Wall time: 4.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X, X_test, y, y_test = load_gpu_data(fpath=data_path,\n",
    "                                             colsToDrop=colsToDropTraining,\n",
    "                                             dataType=cp.float32,\n",
    "                                             cpu=False,\n",
    "                                             splitXY=True,\n",
    "                                             imbalance=False,\n",
    "                                             trainTestSplit=True)\n",
    "X = X.iloc[:num_datapoints,:] \n",
    "y = y.iloc[:num_datapoints] \n",
    "\n",
    "X_test = X_test.iloc[:num_datapoints,:] \n",
    "y_test = y_test.iloc[:num_datapoints] \n",
    "\n",
    "print(f'data shape: {X.shape}, {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5788ef49-6c9f-4827-825b-8a29a1fbe208",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "no implementation found for 'numpy.where' on types that implement __array_function__: [<class 'cudf.core.series.Series'>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Getting the indices that are associated with land (0) and water (1)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m y_water_ind \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m y_land_ind \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m0.5\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#Subset the X AND y data to later/ subset with the clusters and then combine for RFA\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: no implementation found for 'numpy.where' on types that implement __array_function__: [<class 'cudf.core.series.Series'>]"
     ]
    }
   ],
   "source": [
    "# #Getting the indices that are associated with land (0) and water (1)\n",
    "# y_water_ind = np.where(y>0.5)[0]\n",
    "# y_land_ind = np.where(y<0.5)[0]\n",
    "\n",
    "# #Subset the X AND y data to later/ subset with the clusters and then combine for RFA\n",
    "# X_water = X.iloc[y_water_ind,:]\n",
    "# y_water = y.iloc[y_water_ind]\n",
    "\n",
    "# X_land = X.iloc[y_land_ind,:]\n",
    "# y_land = y.iloc[y_land_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b38dfe2-a25e-4656-a5e8-444addd34398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sur_refl_b01_1\n",
      "sur_refl_b02_1\n",
      "sur_refl_b07_1\n",
      "ndvi\n"
     ]
    }
   ],
   "source": [
    "_ = [print(column) for column in X.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee6bed-e60c-495e-a0e6-9a1cef0bde50",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Comparing targeted and clustered samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82998bd2-524f-439d-bbd8-200287d0faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_VERSION='v4.2.1'\n",
    "training_data_basepath = f'/explore/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/training_data/{DATA_VERSION}'\n",
    "\n",
    "#VERSION 4.2.1\n",
    "TILE_IN = 'Golden'#v4.2.1\n",
    "offsets_indexes = ['x_offset', 'y_offset', 'year', 'julian_day','tileID']\n",
    "\n",
    "glob_string = os.path.join(training_data_basepath,'MOD*{}*.parquet.gzip'.format(TILE_IN))\n",
    "data_paths = sorted([fv for fv in glob.glob(glob_string)])\n",
    "\n",
    "#Only want the one with 4.2.0 because the other file doesnt work. \n",
    "print(data_paths)\n",
    "data_path = data_paths[0]\n",
    "print(data_path)\n",
    "colsToDropTraining = colsToDrop.copy()\n",
    "colsToDropTraining.extend(offsets_indexes)\n",
    "\n",
    "\n",
    "X_target, X_test_target, y_target, y_test_target = load_data(fpath=data_path,\n",
    "                                colsToDrop=colsToDropTraining,\n",
    "                                dataType=DATA_TYPE,\n",
    "                                cpu=True,\n",
    "                                splitXY=True,\n",
    "                                trainTestSplit=True\n",
    "                                )\n",
    "\n",
    "X_target = X_target.iloc[:num_datapoints,:] \n",
    "y_target = y_target.iloc[:num_datapoints] \n",
    "\n",
    "X_test_target = X_test_target.iloc[:num_datapoints,:] \n",
    "y_test_target = y_test_target.iloc[:num_datapoints] \n",
    "\n",
    "print(f'\\n\\ntarget subset data shape: {X_target.shape}, {y_target.shape}')\n",
    "\n",
    "#Getting the indices that are associated with land (0) and water (1)\n",
    "#Subset the X AND y data to later subset with the clusters and then combine for RFA\n",
    "X_water_target = X_target.iloc[np.where(y_target>0.5)[0],:]\n",
    "X_land_target = X_target.iloc[np.where(y_target<0.5)[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e16622-e1b3-45f8-ba03-30c4a4b31418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb3a1ce-bb9e-4789-bbd1-bcfe2c913785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89fdd11d-a3b3-4733-b95f-f5cb8bd80fc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Clustering Data for Input to Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0e0854-39fc-409d-82ce-9d340243778b",
   "metadata": {},
   "source": [
    "Based on the cluster analysis above on 5.03.23, 15 clusters appears to have the most data and exclude outliers so will use that number for selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860899fd-59ae-420d-ac0b-a1ad8940f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_NUM=15\n",
    "\n",
    "common_params = {\n",
    "    \"n_init\": \"auto\",\n",
    "    # \"random_state\": 42,\n",
    "    \"init\":\"random\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fadaa2-f0c0-43d0-8048-de4c7ef01329",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "kme_land_random =  KMeans(n_clusters=CLUSTER_NUM, **common_params).fit(X_land)\n",
    "kmeans_output_land_random = kme_land_random.predict(X_land)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9ac07-2215-4e65-ab83-e67411dec00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "kme_water_random = KMeans(n_clusters=CLUSTER_NUM, **common_params).fit(X_water)\n",
    "kmeans_output_water_random = kme_water_random.predict(X_water)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbf9549-0236-4f29-a8a3-c1a2d0ab60e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Even Balanced Random pulled datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a34ff7-b94d-472b-8e8b-633272252473",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT_EVEN_BALANCE_LAND = np.inf\n",
    "COUNT_EVEN_BALANCE_WATER = np.inf\n",
    "for cluster in np.unique(kmeans_output_water_random):\n",
    "    land_num = len(np.where(kmeans_output_land_random == cluster)[0])\n",
    "    water_num = len(np.where(kmeans_output_water_random == cluster)[0])\n",
    "    if land_num < COUNT_EVEN_BALANCE_LAND: COUNT_EVEN_BALANCE_LAND = land_num\n",
    "    if water_num < COUNT_EVEN_BALANCE_WATER: COUNT_EVEN_BALANCE_WATER = water_num\n",
    "    \n",
    "print(COUNT_EVEN_BALANCE_LAND, COUNT_EVEN_BALANCE_WATER)\n",
    "if COUNT_EVEN_BALANCE_LAND < COUNT_EVEN_BALANCE_WATER:\n",
    "    COUNT = COUNT_EVEN_BALANCE_LAND\n",
    "else: \n",
    "    COUNT = COUNT_EVEN_BALANCE_WATER\n",
    "print(COUNT,COUNT_EVEN_BALANCE_LAND,COUNT_EVEN_BALANCE_WATER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660e6139-cb3b-4595-9469-215a1252db02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "random_ind_land = np.array([])\n",
    "random_ind_water = []\n",
    "\n",
    "for cluster in np.unique(kmeans_output_water_random):\n",
    "    print(f'cluster {cluster}')\n",
    "    cluster_ind_water = np.where(kmeans_output_water_random == cluster)[0]\n",
    "    random_pts_water = np.random.choice(cluster_ind_water,COUNT,replace=False)\n",
    "    max_X_random_water = np.nanmax(X_water['sur_refl_b01_1'].iloc[random_pts_water])\n",
    "    if max_X_random_water < 10000:\n",
    "        random_ind_water = np.append(random_ind_water, random_pts_water)\n",
    "    else: print(f'Cluster {cluster} contains outliers')\n",
    "    \n",
    "    cluster_ind_land = np.where(kmeans_output_land_random == cluster)[0]\n",
    "    random_pts_land = np.random.choice(cluster_ind_land,COUNT,replace=False)\n",
    "    random_ind_land = np.append(random_ind_land, random_pts_land)\n",
    "    \n",
    "random_ind_water = random_ind_water.astype('int')\n",
    "random_ind_land = random_ind_land.astype('int')\n",
    "\n",
    "print(random_ind_water,random_ind_land)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf76286d-7e58-444e-97ef-27cd5fa83dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster_land_random = X_land.iloc[random_ind_land]\n",
    "y_cluster_land_random = y_land.iloc[random_ind_land]\n",
    "X_cluster_water_random = X_water.iloc[random_ind_water]\n",
    "y_cluster_water_random = y_water.iloc[random_ind_water]\n",
    "\n",
    "X_cluster_random = pd.concat([X_cluster_land_random,X_cluster_water_random])\n",
    "y_cluster_random = pd.concat([y_cluster_land_random,y_cluster_water_random])\n",
    "\n",
    "#Combine the data so that we can shuffle the indices and keep the data together that should be\n",
    "All_data_random = pd.concat([X_cluster_random,y_cluster_random],axis=1).sample(frac=1)\n",
    "\n",
    "X_cluster_rfa_random = All_data_random[X_cluster_random.columns]\n",
    "y_cluster_rfa_random = All_data_random['water']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fcfb98-117c-4e5d-a4de-fdf142114f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(X_cluster_land_random),len(X_cluster_water_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e230982-aa17-48ae-ac01-963766ac0309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(All_data_random)\n",
    "print(X_cluster_rfa_random)\n",
    "print(y_cluster_rfa_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a4cca-556f-4e75-bc92-6361608f42c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Percentage Random pulled datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a6a8c5-ad0e-4e6c-89b7-7e75526f654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the clusters: kmeans_output_land and kmeans_output_water\n",
    "# Data: X_water, X_land, y_water, y_land\n",
    "\n",
    "PERCENT_RANDOM_PULL = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee2dae0-b83d-470b-ae1d-daffa1c42816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "random_ind_land = np.array([])\n",
    "random_ind_water = []\n",
    "\n",
    "for cluster in np.unique(kmeans_output_water_random):\n",
    "    print(f'cluster {cluster}')\n",
    "    cluster_ind_water = np.where(kmeans_output_water_random == cluster)[0]\n",
    "    # cluster_ind_water = np.where(bgm_water == cluster)[0]\n",
    "    COUNT_RANDOM_PULL_WATER = int(PERCENT_RANDOM_PULL*len(cluster_ind_water))\n",
    "    random_pts_water = np.random.choice(cluster_ind_water,COUNT_RANDOM_PULL_WATER,replace=False)\n",
    "    max_X_random_water = np.nanmax(X_water['sur_refl_b01_1'].iloc[random_pts_water])\n",
    "    if max_X_random_water < 10000:\n",
    "        random_ind_water = np.append(random_ind_water, random_pts_water)\n",
    "    else: print(f'Cluster {cluster} contains outliers')\n",
    "    \n",
    "    cluster_ind_land = np.where(kmeans_output_land_random == cluster)[0]\n",
    "    # cluster_ind_land = np.where(bgm_land == cluster)[0]\n",
    "    COUNT_RANDOM_PULL_LAND = int(PERCENT_RANDOM_PULL*len(cluster_ind_land))\n",
    "    random_pts_land = np.random.choice(cluster_ind_land,COUNT_RANDOM_PULL_LAND,replace=False)\n",
    "    random_ind_land = np.append(random_ind_land, random_pts_land)\n",
    "    print(f'Pulling {COUNT_RANDOM_PULL_WATER} Water pts and {COUNT_RANDOM_PULL_LAND} Land pts')\n",
    "    print()\n",
    "random_ind_water = random_ind_water.astype('int')\n",
    "random_ind_land = random_ind_land.astype('int')\n",
    "\n",
    "print(random_ind_water,random_ind_land)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd5a1c-fb13-4e2a-800d-21a1864f1ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d307387-dc6d-49ce-be7d-af4346607953",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Total random dataset used for training random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e0e4d-7b22-4fca-a93a-f16d7da8571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster_land_random = X_land.iloc[random_ind_land]\n",
    "y_cluster_land_random = y_land.iloc[random_ind_land]\n",
    "X_cluster_water_random = X_water.iloc[random_ind_water]\n",
    "y_cluster_water_random = y_water.iloc[random_ind_water]\n",
    "\n",
    "X_cluster_random = pd.concat([X_cluster_land_random,X_cluster_water_random])\n",
    "y_cluster_random = pd.concat([y_cluster_land_random,y_cluster_water_random])\n",
    "\n",
    "#Combine the data so that we can shuffle the indices and keep the data together that should be\n",
    "All_data_random = pd.concat([X_cluster_random,y_cluster_random],axis=1).sample(frac=1)\n",
    "\n",
    "X_cluster_rfa_random = All_data_random[X_cluster_random.columns]\n",
    "y_cluster_rfa_random = All_data_random['water']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba98450-3b2d-46b7-8445-94e67036f79e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(X_cluster_land_random),len(X_cluster_water_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce60e95-e471-4c4a-a292-7d3e4152cca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44ed9581-542c-433e-a139-1a45a83d9612",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0fcb22-fca3-4a5d-940f-01139be277d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIGURE_OUTPUT_DIR = ''#/explore/nobackup/projects/ilab/scratch/cssprad1/MODIS_water/code/tmp'\n",
    "# RASTER_OUTPUT_DIR = ''#/explore/nobackup/projects/ilab/scratch/cssprad1/MODIS_water/code/tmp'\n",
    "# MODEL_OUTPUT_DIR = ''#/explore/nobackup/projects/ilab/scratch/cssprad1/MODIS_water/models/'\n",
    "\n",
    "# qaMaskPath = '/explore/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/qa'\n",
    "# waterMaskPath = '/explore/nobackup/people/mcarrol2/MODIS_water/v5_outputs'\n",
    "\n",
    "# test_data_basepath = '/explore/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/test_data/'\n",
    "\n",
    "# ##Add in the other stuff for tuning\n",
    "# frac_water = int(100*(1.0-FRAC_LAND))\n",
    "# if len(colsToDrop) >= 1:\n",
    "#     save_file = f\"{TILE}_{DAY}_{YEAR}_frac-water{frac_water}_no-vars_{'-'.join(colsToDrop)}\"\n",
    "# else: \n",
    "#      save_file = f\"{TILE}_{DAY}_{YEAR}_frac-water{frac_water}_all-vars\"\n",
    "\n",
    "# print(save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc7e41-cd94-4ca8-8ed3-df7bd2568922",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### RF FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3244e03-63f5-45f2-b28d-dfcd6b7bc8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_rf_objective(trial):\n",
    "    list_trees = [75, 100, 125, 150, 175, 200, 250, 300, 400, 500]\n",
    "    max_depth = [5, 10, 30, 50, 80, 90, 100, 110]\n",
    "    min_samples_leaf = [1, 2, 3, 4, 5]\n",
    "    min_samples_split = [2, 4, 8, 10]\n",
    "    bootstrap = [True, False]\n",
    "    max_features = ['auto', 'sqrt', 'log2']\n",
    "    \n",
    "    # if TREES_AND_DEPTH_ONLY:\n",
    "    #     param = {'n_estimators': trial.suggest_categorical('n_estimators', list_trees), \n",
    "    #                'criterion':'gini', \n",
    "    #                'max_depth':trial.suggest_categorical('max_depth', max_depth), \n",
    "    #                'min_samples_split':2, \n",
    "    #                'min_samples_leaf':1, \n",
    "    #                'min_weight_fraction_leaf':0.0, \n",
    "    #                'max_features':'auto', \n",
    "    #                'max_leaf_nodes':None, \n",
    "    #                'min_impurity_decrease':0.0, \n",
    "    #                'bootstrap':True, \n",
    "    #                'oob_score':False, \n",
    "    #                'n_jobs':-1, \n",
    "    #                # 'random_state':42, \n",
    "    #                'verbose':0, \n",
    "    #                'warm_start':True, \n",
    "    #                'class_weight':None, \n",
    "    #                'ccp_alpha':0.0, \n",
    "    #                'max_samples':None\n",
    "    #                   }\n",
    "    # else:\n",
    "    param = {'n_estimators': trial.suggest_categorical('n_estimators', list_trees), \n",
    "                       'max_depth':trial.suggest_categorical('max_depth', max_depth), \n",
    "                       'min_samples_split':trial.suggest_categorical('min_samples_split', min_samples_split), \n",
    "                       'min_samples_leaf':trial.suggest_categorical('min_samples_leaf', min_samples_leaf), \n",
    "                       'bootstrap': trial.suggest_categorical('bootstrap', bootstrap),\n",
    "                       'criterion':'gini', \n",
    "                       #'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 1e-8, 1.0, log=True), \n",
    "                       'max_features':trial.suggest_categorical('max_features', max_features), \n",
    "                       'max_leaf_nodes':None, \n",
    "                       'min_impurity_decrease':0.0, \n",
    "                       'oob_score':False, \n",
    "                       'n_jobs':-1, \n",
    "                       # 'random_state':42, \n",
    "                       'verbose':0, \n",
    "                       'warm_start':False, \n",
    "                       'class_weight':None, \n",
    "                       'ccp_alpha':0.0, \n",
    "                       'max_samples':None\n",
    "                      }\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    #######################\n",
    "    # HERE IS WHERE TO CHANGE THE X,Y DATASET USED FOR TRAINING\n",
    "    #######################\n",
    "   \n",
    "    cv_scores = np.empty(5)\n",
    "    # for idx, (train_idx, val_idx) in enumerate(cv.split(X,y)):\n",
    "    #     # X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    #     # y_train, y_val = y.iloc[train_idx],  y.iloc[val_idx]\n",
    "\n",
    "    for idx, (train_idx, val_idx) in enumerate(cv.split(X_cluster_rfa_random,  y_cluster_rfa_random)):    \n",
    "        X_train, X_val = X_cluster_rfa_random.iloc[train_idx], X_cluster_rfa_random.iloc[val_idx]\n",
    "        y_train, y_val = y_cluster_rfa_random.iloc[train_idx],  y_cluster_rfa_random.iloc[val_idx]\n",
    "        \n",
    "    # for idx, (train_idx, val_idx) in enumerate(cv.split(X_cluster_rfa_center, y_cluster_rfa_center)):\n",
    "    #     X_train, X_val = X_cluster_rfa_center.iloc[train_idx], X_cluster_rfa_center.iloc[val_idx]\n",
    "    #     y_train, y_val = y_cluster_rfa_center.iloc[train_idx], y_cluster_rfa_center.iloc[val_idx]\n",
    "\n",
    "        model = skRF(**param)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_val)\n",
    "        cv_scores[idx] = f1_score(y_val, preds)\n",
    "        if cv_scores[idx] == 0.0:\n",
    "            print('Pruning because of 0.0 score.')\n",
    "            return 0.0\n",
    "        print('Fold {}: {}'.format(idx, cv_scores[idx]))\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "search_space={\n",
    "    \"n_estimators\": [75, 100, 125, 150, 175, 200, 250, 300, 400, 500],\n",
    "    \"max_depth\" : [5,10, 30, 50, 80, 90, 100, 110],\n",
    "    \"min_samples_leaf\" : [1, 2, 3, 4, 5],\n",
    "    \"min_samples_split\" : [2, 4, 8, 10],\n",
    "    \"bootstrap\" : [True, False],\n",
    "    \"max_features\" : ['auto', 'sqrt', 'log2'],\n",
    "    \n",
    "}\n",
    "TREES_AND_DEPTH_ONLY = False\n",
    "GRID_SEARCH = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4014f162-87bd-438d-aa54-c07651dab57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         sur_refl_b01_1  sur_refl_b02_1  sur_refl_b07_1          ndvi\n",
      "2503850          1932.0          3032.0          2755.0   2215.954834\n",
      "5087060          1104.0          2616.0          2286.0   4064.516113\n",
      "3343370            17.0            11.0             1.0  -2142.857178\n",
      "2091537           398.0          2522.0           924.0   7273.972656\n",
      "1744767           198.0           170.0            80.0   -760.869568\n",
      "...                 ...             ...             ...           ...\n",
      "1292987           556.0          2078.0          1177.0   5778.284180\n",
      "4525991            13.0             1.0            43.0  -8571.428711\n",
      "2615424          1170.0          3158.0          2104.0   4593.345703\n",
      "1222149            34.0             2.0            20.0  -8888.888672\n",
      "3752190             3.0           -15.0            13.0  15000.000000\n",
      "\n",
      "[4375821 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0070240-5655-486d-9011-b038ebee4403",
   "metadata": {},
   "source": [
    "#### Training RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca239ea1-6081-466a-9458-c158eb7f2393",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-01 13:44:42,756]\u001b[0m A new study created in memory with name: RF Tuning\u001b[0m\n",
      "\u001b[33m[W 2023-08-01 13:44:42,759]\u001b[0m Trial 0 failed with parameters: {'n_estimators': 100, 'max_depth': 30} because of the following error: TypeError('Implicit conversion to a host NumPy array via __array__ is not allowed, To explicitly construct a GPU matrix, consider using .to_cupy()\\nTo explicitly construct a host matrix, consider using .to_numpy().').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_2599613/1274776373.py\", line 27, in gpu_rf_objective\n",
      "    for idx, (train_idx, val_idx) in enumerate(cv.split(X,y)):\n",
      "  File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/sklearn/model_selection/_split.py\", line 771, in split\n",
      "    y = check_array(y, input_name=\"y\", ensure_2d=False, dtype=None)\n",
      "  File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 879, in check_array\n",
      "    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n",
      "  File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n",
      "    array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "  File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/cudf/core/frame.py\", line 451, in __array__\n",
      "    raise TypeError(\n",
      "TypeError: Implicit conversion to a host NumPy array via __array__ is not allowed, To explicitly construct a GPU matrix, consider using .to_cupy()\n",
      "To explicitly construct a host matrix, consider using .to_numpy().\n",
      "\u001b[33m[W 2023-08-01 13:44:42,761]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Implicit conversion to a host NumPy array via __array__ is not allowed, To explicitly construct a GPU matrix, consider using .to_cupy()\nTo explicitly construct a host matrix, consider using .to_numpy().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:17\u001b[0m\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/optuna/study/study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[33], line 27\u001b[0m, in \u001b[0;36mgpu_rf_objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     25\u001b[0m cv \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     26\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (train_idx, val_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m     28\u001b[0m     X_train, X_val \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc[train_idx], X\u001b[38;5;241m.\u001b[39miloc[val_idx]\n\u001b[1;32m     29\u001b[0m     y_train, y_val \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39miloc[train_idx], y\u001b[38;5;241m.\u001b[39miloc[val_idx]\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/sklearn/model_selection/_split.py:771\u001b[0m, in \u001b[0;36mStratifiedKFold.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    738\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \n\u001b[1;32m    740\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;124;03m    to an integer.\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 771\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/sklearn/utils/validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    883\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/sklearn/utils/_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    182\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/cudf/core/frame.py:451\u001b[0m, in \u001b[0;36mFrame.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplicit conversion to a host NumPy array via __array__ is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallowed, To explicitly construct a GPU matrix, consider using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.to_cupy()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTo explicitly construct a host matrix, consider \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing .to_numpy().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    456\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Implicit conversion to a host NumPy array via __array__ is not allowed, To explicitly construct a GPU matrix, consider using .to_cupy()\nTo explicitly construct a host matrix, consider using .to_numpy()."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "if GRID_SEARCH:\n",
    "    study = optuna.create_study(study_name='RF Tuning Grid Search', \n",
    "                                direction='maximize',\n",
    "                                sampler=optuna.samplers.GridSampler(search_space))\n",
    "    \n",
    "else:\n",
    "    study = optuna.create_study(study_name='RF Tuning',\n",
    "                                direction='maximize')\n",
    "#Objective is under the functions area\n",
    "\n",
    "#####################################################################\n",
    "#CHANGE HERE FOR DIFFERENT MODELING TYPE\n",
    "#rf_objective or xgb_objective\n",
    "#####################################################################\n",
    "\n",
    "study.optimize(cpu_rf_objective, n_trials=25, timeout=30*600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b219a2ca-012b-4fea-804d-2bdac8cec747",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "trials = study.best_trials\n",
    "trial_score = max([trial.values[0]\n",
    "                   for trial in trials])\n",
    "best_trial_params = [trial.params for trial in trials if trial.values[0] == trial_score][0]\n",
    "print(best_trial_params)\n",
    "print(trial_score)\n",
    "score_print = np.round(trial_score,4)\n",
    "print(score_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a5d73-83ed-4c20-a9b1-d637d9ac0791",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_cluster_land_random),len(X_cluster_water_random),trial_score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb27e0f-3ab7-41f8-bda7-2c0e7564578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = best_trial_params\n",
    "hyperparameters['n_jobs'] = -1\n",
    "print('Using these params:')\n",
    "print(hyperparameters)\n",
    "tuned_classifier = skRF(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dea7c9-7927-4f2c-9b11-8bfa0dd082ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "tuned_classifier.fit(X_cluster_rfa_random , y_cluster_rfa_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29964f62-43bb-453c-9b65-a7526af71bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'rfa_models/MODIS_RFA_v201_PercentCluster_sfcref127ndvi_10.pkl'\n",
    "print(filename)\n",
    "pickle.dump(tuned_classifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3220fba-351e-4bc5-9e24-f2af5dcf901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_model = pickle.load(open('rfa_models/MODIS_RFA_v201_EBCluster_sfcref127ndvi_4.pkl', 'rb'))\n",
    "print(pickled_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee12a19-8cf8-472d-ba4d-e269e7356272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EB1: 0.9121361508267432\n",
    "# EB2: 0.9152120215067565\n",
    "# EB3: 0.9153888116216589\n",
    "# EB4: 0.9334065797136774\n",
    "# EB5: 0.9330483639924072\n",
    "# EB6: 0.9325007563612594\n",
    "# EB7: 13875 12950 0.9330562509477165\n",
    "# EB8: 14220 13272 0.9119937102685194 \n",
    "# EB9: 13875 12950 0.9318627771973583\n",
    "# EB10: 14220 13272 0.9128319097402475\n",
    "\n",
    "# %1: 359648 295422 0.9778067788170863\n",
    "# %2: 359649 295421 0.977828848863204\n",
    "# %3: 359649 295421 0.977814397217147\n",
    "# %4: 359648 295421 0.9777926266177195\n",
    "# %5: 359650 295422 0.9778923365411023\n",
    "# %6: 359843 295233 0.9779668774041262\n",
    "# %7: 359841 295230 0.977761799842581\n",
    "# %8: 359840 296518 0.977231470922011\n",
    "# %9: 359835 295223 0.977650365098458\n",
    "# %10: 359837 295222 0.9778535497660246\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ILAB Kernel (TensorFlow)",
   "language": "python",
   "name": "tensorflow-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
