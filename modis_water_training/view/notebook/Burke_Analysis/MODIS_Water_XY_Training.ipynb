{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b714c30-b71c-43c6-9fe3-9b012a7bfdfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  MODIS Water Cluster Training\n",
    "\n",
    "Version: 0.1.0\n",
    "\n",
    "Date modified: 05.01.2023\n",
    "\n",
    "Modified by: Amanda Burke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ef8ad40-b2c6-4ccc-9a4b-c595bcce20e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "import math \n",
    "import pandas as pd\n",
    "from pathlib import Path   \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "\n",
    "# plt.style.use('fivethirtyeight')\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier as skRF\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, precision_score, f1_score\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, matthews_corrcoef\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold, StratifiedKFold\n",
    "\n",
    "# GPU-based frameworks\n",
    "import cudf\n",
    "import cupy as cp\n",
    "from cuml.ensemble import RandomForestClassifier as cuRFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169c7a95-24a1-4ceb-9036-27369d259e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef09a8be-17ed-40a8-958e-ed66b83929bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'rf'\n",
    "TEST_RATIO = 0.2\n",
    "RANDOM_STATE = 42\n",
    "LABEL_NAME = 'water'\n",
    "if GPU is False:\n",
    "    DATA_TYPE = np.int16\n",
    "else: \n",
    "    DATA_TYPE = cp.float32\n",
    "FRAC_LAND=0.5\n",
    "num_datapoints = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bb0fe47-657c-4bdd-b41d-cb8e70068281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/explore/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/training_data/v4.2.1/MOD09_Golden_Masked_986161_4_2_1.parquet.gzip']\n",
      "/explore/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/training_data/v4.2.1/MOD09_Golden_Masked_986161_4_2_1.parquet.gzip\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# VERSION 4.2.1 (targeted 500k points)\n",
    "TILE_IN = 'Golden'#v4.2.1\n",
    "DATA_VERSION='v4.2.1'\n",
    "offsets_indexes = ['x_offset', 'y_offset', 'year', 'julian_day','tileID']\n",
    "#############################\n",
    "\n",
    "# ##############################\n",
    "# #VERSION 2.0.1 (5 million points)\n",
    "# TILE_IN = 'GLOBAL'#v2.0.1\n",
    "# DATA_VERSION='v2.0.1'\n",
    "# offsets_indexes = ['x_offset', 'y_offset', 'year', 'julian_day']\n",
    "# ##############################\n",
    "\n",
    "# #############################\n",
    "# #VERSION 0.0.0 (2billion data points)\n",
    "# TILE_IN = 'cleaned'#v2.0.1\n",
    "# DATA_VERSION='AGU'\n",
    "# offsets_indexes = []#'x_offset', 'y_offset', 'year', 'julian_day']\n",
    "# ##############################\n",
    "\n",
    "training_data_basepath = f'/explore/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/training_data/{DATA_VERSION}'\n",
    "glob_string = os.path.join(training_data_basepath,'MOD*{}*.parquet.gzip'.format(TILE_IN))\n",
    "data_paths = sorted([fv for fv in glob.glob(glob_string)])\n",
    "\n",
    "#Only want the one with 4.2.0 because the other file doesnt work. \n",
    "print(data_paths)\n",
    "data_path = data_paths[0]\n",
    "print(data_path)\n",
    "\n",
    "colsToDrop = [\n",
    "    # 'sur_refl_b01_1',\n",
    "    # 'sur_refl_b02_1',\n",
    "    'sur_refl_b03_1',\n",
    "    'sur_refl_b04_1','sur_refl_b05_1','sur_refl_b06_1',\n",
    "    # 'sur_refl_b07_1',\n",
    "    # 'ndvi',\n",
    "    'ndwi1','ndwi2'\n",
    "        ]\n",
    "\n",
    "colsToDropTraining = colsToDrop.copy()\n",
    "colsToDropTraining.extend(offsets_indexes)\n",
    "v_names = ['sur_refl_b01_1','sur_refl_b02_1','sur_refl_b03_1',\n",
    "           'sur_refl_b04_1','sur_refl_b05_1','sur_refl_b06_1',\n",
    "           'sur_refl_b07_1','ndvi','ndwi1','ndwi2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd27567-2a51-4e94-8d0a-43504a90bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cpu_data(fpath, colsToDrop, yCol='water', testSize=0.2, randomState=42, \n",
    "            dataType=np.float32, cpu=True, splitXY=False, trainTestSplit=False,\n",
    "            applyLog=False, imbalance=False, frac=0.1, land=False, multi=False, \n",
    "            multisample=1000000):\n",
    "    \"\"\"\n",
    "    Simple helper function for loading data to be used by models\n",
    "    :param fpath: Path to the data to be ingested.\n",
    "    :param dataType: Data type to convert ingested data to.\n",
    "    :param colsToDrop: Columns which are not necessary, from which to drop.\n",
    "    :param testSize: Ration to\n",
    "    \"\"\"\n",
    "    if multi:\n",
    "        all_dfs = [pd.read_csv(path_) for path_ in fpath]\n",
    "        df = pd.concat(all_dfs).sample(n=multisample, random_state=randomState)\n",
    "        print('DF length: {}'.format(len(df.index)))\n",
    "    else:   \n",
    "        df = pd.read_parquet(fpath) if '.parquet' in fpath else pd.read_csv(fpath)\n",
    "    df = df[df['sur_refl_b01_1'] + df['sur_refl_b02_1'] != 0]\n",
    "    df = df[df['sur_refl_b07_1'] + df['sur_refl_b02_1'] != 0]\n",
    "    df = df[df['sur_refl_b06_1'] + df['sur_refl_b02_1'] != 0]\n",
    "\n",
    "    df = df.drop(columns=colsToDrop)\n",
    "    cleanedDF = df[~df.isin([np.NaN, np.inf, -np.inf]).any(1)].dropna(axis=0).astype(dataType)\n",
    "    if applyLog:\n",
    "        for col in cleanedDF.drop([yCol], axis=1).columns:\n",
    "            print('Applying log1p func to {}'.format(col))\n",
    "            cleanedDF[col] = np.log1p(cleanedDF[col])\n",
    "        cleanedDF = cleanedDF[~cleanedDF.isin([np.NaN, np.inf, -np.inf]).any(1)].dropna(axis=0)\n",
    "    df = None\n",
    "    if imbalance:\n",
    "        if land:\n",
    "            print('Imbalancing data, sampling {} from water'.format(frac))\n",
    "        else:\n",
    "            print(f'Imbalancing data, sampling {frac} from land, {1-frac} from water')\n",
    "        groupedDF = cleanedDF.groupby('water')\n",
    "        dfs = [groupedDF.get_group(y) for y in groupedDF.groups]\n",
    "        sampledDF = dfs[1].sample(frac=frac)if land else dfs[0].sample(frac=frac)\n",
    "        concatDF = sampledDF.append(dfs[0]) if land else sampledDF.append(dfs[1])\n",
    "        concatDF = concatDF.sample(frac=1)\n",
    "        concatDF = concatDF.reset_index()\n",
    "        cleanedDF = concatDF.drop(columns=['index'])\n",
    "    if not splitXY:\n",
    "        return cleanedDF\n",
    "    X = cleanedDF.drop([yCol], axis=1).astype(dataType)\n",
    "    y = cleanedDF[yCol].astype(dataType)\n",
    "    if trainTestSplit:\n",
    "        return train_test_split(X, y, test_size=TEST_RATIO)\n",
    "    else:\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cea32a90-09e3-4d33-8a91-c62cecaa548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gpu_data(fpath, colsToDrop, yCol='water', testSize=0.2, randomState=42, \n",
    "            dataType=cp.float32, cpu=False, splitXY=True, trainTestSplit=True,\n",
    "            applyLog=False, imbalance=False, frac=0.1, land=False, multi=False, \n",
    "            multisample=1000000):\n",
    "    \"\"\"\n",
    "    Simple helper function for loading data to be used by models\n",
    "    :param fpath: Path to the data to be ingested.\n",
    "    :param dataType: Data type to convert ingested data to.\n",
    "    :param colsToDrop: Columns which are not necessary, from which to drop.\n",
    "    :param testSize: Ration to\n",
    "    \"\"\"\n",
    "    if multi:\n",
    "        all_dfs = [pd.read_csv(path_) for path_ in fpath]\n",
    "        df = pd.concat(all_dfs).sample(n=multisample, random_state=randomState)\n",
    "        print('DF length: {}'.format(len(df.index)))\n",
    "    else:   \n",
    "        df = pd.read_parquet(fpath) if '.parquet' in fpath else pd.read_csv(fpath)\n",
    "    df = df[df['sur_refl_b01_1'] + df['sur_refl_b02_1'] != 0]\n",
    "    df = df[df['sur_refl_b07_1'] + df['sur_refl_b02_1'] != 0]\n",
    "    df = df[df['sur_refl_b06_1'] + df['sur_refl_b02_1'] != 0]\n",
    "    df = df.drop(columns=colsToDrop)\n",
    "    cleanedDF = df[~df.isin([np.NaN, np.inf, -np.inf]).any(1)].dropna(axis=0).astype(dataType)\n",
    "    cleanedDF = cudf.from_pandas(cleanedDF) if not cpu else cleanedDF\n",
    "    if applyLog:\n",
    "        for col in cleanedDF.drop([yCol], axis=1).columns:\n",
    "            print('Applying log1p func to {}'.format(col))\n",
    "            cleanedDF[col] = np.log1p(cleanedDF[col])\n",
    "        cleanedDF = cleanedDF[~cleanedDF.isin([np.NaN, np.inf, -np.inf]).any(1)].dropna(axis=0)\n",
    "    df = None\n",
    "    if imbalance:\n",
    "        if land:\n",
    "            print('Imbalancing data, sampling {} from water'.format(frac))\n",
    "        else:\n",
    "            print('Imbalancing data, sampling {} from land'.format(frac))\n",
    "        groupedDF = cleanedDF.groupby('water')\n",
    "        dfs = [groupedDF.get_group(y) for y in groupedDF.groups]\n",
    "        sampledDF = dfs[1].sample(frac=frac)if land else dfs[0].sample(frac=frac)\n",
    "        concatDF = sampledDF.append(dfs[0]) if land else sampledDF.append(dfs[1])\n",
    "        concatDF = concatDF.sample(frac=1)\n",
    "        concatDF = concatDF.reset_index()\n",
    "        cleanedDF = concatDF.drop(columns=['index'])\n",
    "    if not splitXY:\n",
    "        return cleanedDF\n",
    "    X = cleanedDF.drop([yCol], axis=1).astype(dataType)\n",
    "    y = cleanedDF[yCol].astype(dataType)\n",
    "    cleanedX = cleanedDF.drop([yCol], axis=1).astype(dataType)\n",
    "    cleanedy = cleanedDF[yCol].astype(dataType)\n",
    "    if trainTestSplit:\n",
    "        return train_test_split(cleanedX, cleanedy, test_size=TEST_RATIO)\n",
    "    else:\n",
    "        return cleanedX, cleanedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdee2440-7931-4da3-80e7-f79aefcb15dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "133868b4-66bb-4f85-bcb2-641a79f06e2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b8a3852-14ee-4615-bc7c-871046ca19e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sur_refl_b03_1',\n",
       " 'sur_refl_b04_1',\n",
       " 'sur_refl_b05_1',\n",
       " 'sur_refl_b06_1',\n",
       " 'ndwi1',\n",
       " 'ndwi2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colsToDrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb281755-23fe-4789-ba8b-584105111691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (788928, 4), (788928,)\n",
      "CPU times: user 2.87 s, sys: 2.24 s, total: 5.11 s\n",
      "Wall time: 32.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "load_data_params = {'fpath':data_path,'colsToDrop':colsToDropTraining,'splitXY':True,\n",
    "                    'imbalance':False,'trainTestSplit':True}\n",
    "\n",
    "# print(load_data_params)\n",
    "\n",
    "if GPU is False: \n",
    "    X, X_test, y, y_test = load_cpu_data(**load_data_params)\n",
    "else: \n",
    "    X, X_test, y, y_test = load_gpu_data(**load_data_params)\n",
    "        # fpath=data_path,\n",
    "        #  colsToDrop=colsToDropTraining,\n",
    "        #  splitXY=True,\n",
    "        #  imbalance=False,\n",
    "        #  trainTestSplit=True)\n",
    "        \n",
    "X = X.iloc[:num_datapoints,:] \n",
    "y = y.iloc[:num_datapoints] \n",
    "\n",
    "X_test = X_test.iloc[:num_datapoints,:] \n",
    "y_test = y_test.iloc[:num_datapoints] \n",
    "\n",
    "print(f'data shape: {X.shape}, {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a444a79c-1f48-4093-8a74-63636ca44092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sur_refl_b01_1</th>\n",
       "      <th>sur_refl_b02_1</th>\n",
       "      <th>sur_refl_b07_1</th>\n",
       "      <th>ndvi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>673826</th>\n",
       "      <td>34.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-12666.666992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296281</th>\n",
       "      <td>1551.0</td>\n",
       "      <td>1834.0</td>\n",
       "      <td>1411.0</td>\n",
       "      <td>836.041382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12470</th>\n",
       "      <td>583.0</td>\n",
       "      <td>2036.0</td>\n",
       "      <td>976.0</td>\n",
       "      <td>5547.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900642</th>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>-10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777584</th>\n",
       "      <td>416.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-4857.143066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158742</th>\n",
       "      <td>501.0</td>\n",
       "      <td>1514.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>5027.295410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779159</th>\n",
       "      <td>2874.0</td>\n",
       "      <td>2889.0</td>\n",
       "      <td>1977.0</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879627</th>\n",
       "      <td>25.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-13809.523438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443314</th>\n",
       "      <td>322.0</td>\n",
       "      <td>3237.0</td>\n",
       "      <td>753.0</td>\n",
       "      <td>8190.502930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479057</th>\n",
       "      <td>739.0</td>\n",
       "      <td>2886.0</td>\n",
       "      <td>1239.0</td>\n",
       "      <td>5922.758789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>788928 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sur_refl_b01_1  sur_refl_b02_1  sur_refl_b07_1          ndvi\n",
       "673826            34.0            -4.0             1.0 -12666.666992\n",
       "296281          1551.0          1834.0          1411.0    836.041382\n",
       "12470            583.0          2036.0           976.0   5547.000000\n",
       "900642            34.0             0.0            54.0 -10000.000000\n",
       "777584           416.0           144.0            30.0  -4857.143066\n",
       "...                ...             ...             ...           ...\n",
       "158742           501.0          1514.0           231.0   5027.295410\n",
       "779159          2874.0          2889.0          1977.0     26.000000\n",
       "879627            25.0            -4.0            20.0 -13809.523438\n",
       "443314           322.0          3237.0           753.0   8190.502930\n",
       "479057           739.0          2886.0          1239.0   5922.758789\n",
       "\n",
       "[788928 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5788ef49-6c9f-4827-825b-8a29a1fbe208",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "no implementation found for 'numpy.where' on types that implement __array_function__: [<class 'cudf.core.series.Series'>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Getting the indices that are associated with land (0) and water (1)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m y_water_ind \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m y_land_ind \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m0.5\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#Subset the X AND y data to later/ subset with the clusters and then combine for RFA\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: no implementation found for 'numpy.where' on types that implement __array_function__: [<class 'cudf.core.series.Series'>]"
     ]
    }
   ],
   "source": [
    "#Getting the indices that are associated with land (0) and water (1)\n",
    "y_water_ind = np.where(y>0.5)[0]\n",
    "y_land_ind = np.where(y<0.5)[0]\n",
    "\n",
    "#Subset the X AND y data to later/ subset with the clusters and then combine for RFA\n",
    "X_water = X.iloc[y_water_ind,:]\n",
    "y_water = y.iloc[y_water_ind]\n",
    "\n",
    "X_land = X.iloc[y_land_ind,:]\n",
    "y_land = y.iloc[y_land_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b38dfe2-a25e-4656-a5e8-444addd34398",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [print(column) for column in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82998bd2-524f-439d-bbd8-200287d0faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_VERSION='v4.2.1'\n",
    "# training_data_basepath = f'/explore/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/training_data/{DATA_VERSION}'\n",
    "\n",
    "# #VERSION 4.2.1\n",
    "# TILE_IN = 'Golden'#v4.2.1\n",
    "# offsets_indexes = ['x_offset', 'y_offset', 'year', 'julian_day','tileID']\n",
    "\n",
    "# glob_string = os.path.join(training_data_basepath,'MOD*{}*.parquet.gzip'.format(TILE_IN))\n",
    "# data_paths = sorted([fv for fv in glob.glob(glob_string)])\n",
    "\n",
    "# #Only want the one with 4.2.0 because the other file doesnt work. \n",
    "# print(data_paths)\n",
    "# data_path = data_paths[0]\n",
    "# print(data_path)\n",
    "# colsToDropTraining = colsToDrop.copy()\n",
    "# colsToDropTraining.extend(offsets_indexes)\n",
    "\n",
    "\n",
    "# X_target, X_test_target, y_target, y_test_target = load_data(fpath=data_path,\n",
    "#                                 colsToDrop=colsToDropTraining,\n",
    "#                                 dataType=DATA_TYPE,\n",
    "#                                 cpu=True,\n",
    "#                                 splitXY=True,\n",
    "#                                 trainTestSplit=True\n",
    "#                                 )\n",
    "\n",
    "# X_target = X_target.iloc[:num_datapoints,:] \n",
    "# y_target = y_target.iloc[:num_datapoints] \n",
    "\n",
    "# X_test_target = X_test_target.iloc[:num_datapoints,:] \n",
    "# y_test_target = y_test_target.iloc[:num_datapoints] \n",
    "\n",
    "# print(f'\\n\\ntarget subset data shape: {X_target.shape}, {y_target.shape}')\n",
    "\n",
    "# #Getting the indices that are associated with land (0) and water (1)\n",
    "# #Subset the X AND y data to later subset with the clusters and then combine for RFA\n",
    "# X_water_target = X_target.iloc[np.where(y_target>0.5)[0],:]\n",
    "# X_land_target = X_target.iloc[np.where(y_target<0.5)[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e16622-e1b3-45f8-ba03-30c4a4b31418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb3a1ce-bb9e-4789-bbd1-bcfe2c913785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89fdd11d-a3b3-4733-b95f-f5cb8bd80fc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Clustering Data for Input to Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0e0854-39fc-409d-82ce-9d340243778b",
   "metadata": {},
   "source": [
    "Based on the cluster analysis above on 5.03.23, 15 clusters appears to have the most data and exclude outliers so will use that number for selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860899fd-59ae-420d-ac0b-a1ad8940f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_NUM=15\n",
    "\n",
    "common_params = {\n",
    "    \"n_init\": \"auto\",\n",
    "    \"random_state\": 42,\n",
    "    \"init\":\"random\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fadaa2-f0c0-43d0-8048-de4c7ef01329",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "kme_land_random =  KMeans(n_clusters=CLUSTER_NUM, **common_params).fit(X_land)\n",
    "kmeans_output_land_random = kme_land_random.predict(X_land)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9ac07-2215-4e65-ab83-e67411dec00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "kme_water_random = KMeans(n_clusters=CLUSTER_NUM, **common_params).fit(X_water)\n",
    "kmeans_output_water_random = kme_water_random.predict(X_water)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbf9549-0236-4f29-a8a3-c1a2d0ab60e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Even Balanced Random pulled datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a34ff7-b94d-472b-8e8b-633272252473",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT_EVEN_BALANCE_LAND = np.inf\n",
    "COUNT_EVEN_BALANCE_WATER = np.inf\n",
    "for cluster in np.unique(kmeans_output_water_random):\n",
    "    land_num = len(np.where(kmeans_output_land_random == cluster)[0])\n",
    "    water_num = len(np.where(kmeans_output_water_random == cluster)[0])\n",
    "    if land_num < COUNT_EVEN_BALANCE_LAND: COUNT_EVEN_BALANCE_LAND = land_num\n",
    "    if water_num < COUNT_EVEN_BALANCE_WATER: COUNT_EVEN_BALANCE_WATER = water_num\n",
    "    \n",
    "print(COUNT_EVEN_BALANCE_LAND, COUNT_EVEN_BALANCE_WATER)\n",
    "if COUNT_EVEN_BALANCE_LAND < COUNT_EVEN_BALANCE_WATER:\n",
    "    COUNT = COUNT_EVEN_BALANCE_LAND\n",
    "else: \n",
    "    COUNT = COUNT_EVEN_BALANCE_WATER\n",
    "print(COUNT,COUNT_EVEN_BALANCE_LAND,COUNT_EVEN_BALANCE_WATER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660e6139-cb3b-4595-9469-215a1252db02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "random_ind_land = np.array([])\n",
    "random_ind_water = []\n",
    "\n",
    "for cluster in np.unique(kmeans_output_water_random):\n",
    "    print(f'cluster {cluster}')\n",
    "    cluster_ind_water = np.where(kmeans_output_water_random == cluster)[0]\n",
    "    random_pts_water = np.random.choice(cluster_ind_water,COUNT,replace=False)\n",
    "    max_X_random_water = np.nanmax(X_water['sur_refl_b01_1'].iloc[random_pts_water])\n",
    "    if max_X_random_water < 10000:\n",
    "        random_ind_water = np.append(random_ind_water, random_pts_water)\n",
    "    else: print(f'Cluster {cluster} contains outliers')\n",
    "    \n",
    "    cluster_ind_land = np.where(kmeans_output_land_random == cluster)[0]\n",
    "    random_pts_land = np.random.choice(cluster_ind_land,COUNT,replace=False)\n",
    "    random_ind_land = np.append(random_ind_land, random_pts_land)\n",
    "    \n",
    "random_ind_water = random_ind_water.astype('int')\n",
    "random_ind_land = random_ind_land.astype('int')\n",
    "\n",
    "print(random_ind_water,random_ind_land)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559df0b1-19f7-41be-a4f0-7cf5d3964ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e230982-aa17-48ae-ac01-963766ac0309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f22a4cca-556f-4e75-bc92-6361608f42c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Percentage Random pulled datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a6a8c5-ad0e-4e6c-89b7-7e75526f654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the clusters: kmeans_output_land and kmeans_output_water\n",
    "# Data: X_water, X_land, y_water, y_land\n",
    "\n",
    "PERCENT_RANDOM_PULL = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa603ad-b373-4797-b1b2-cd966ef1c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "random_ind_land = np.array([])\n",
    "random_ind_water = []\n",
    "\n",
    "for cluster in np.unique(kmeans_output_water_random):\n",
    "    print(f'cluster {cluster}')\n",
    "    cluster_ind_water = np.where(kmeans_output_water_random == cluster)[0]\n",
    "    # cluster_ind_water = np.where(bgm_water == cluster)[0]\n",
    "    COUNT_RANDOM_PULL_WATER = int(PERCENT_RANDOM_PULL*len(cluster_ind_water))\n",
    "    random_pts_water = np.random.choice(cluster_ind_water,COUNT_RANDOM_PULL_WATER,replace=False)\n",
    "    max_X_random_water = np.nanmax(X_water['sur_refl_b01_1'].iloc[random_pts_water])\n",
    "    if max_X_random_water < 10000:\n",
    "        random_ind_water = np.append(random_ind_water, random_pts_water)\n",
    "    else: print(f'Cluster {cluster} contains outliers')\n",
    "    \n",
    "    cluster_ind_land = np.where(kmeans_output_land_random == cluster)[0]\n",
    "    # cluster_ind_land = np.where(bgm_land == cluster)[0]\n",
    "    COUNT_RANDOM_PULL_LAND = int(PERCENT_RANDOM_PULL*len(cluster_ind_land))\n",
    "    random_pts_land = np.random.choice(cluster_ind_land,COUNT_RANDOM_PULL_LAND,replace=False)\n",
    "    random_ind_land = np.append(random_ind_land, random_pts_land)\n",
    "    # print(f'Pulling {COUNT_RANDOM_PULL_WATER} Water pts and {COUNT_RANDOM_PULL_LAND} Land pts')\n",
    "    # print()\n",
    "random_ind_water = random_ind_water.astype('int')\n",
    "random_ind_land = random_ind_land.astype('int')\n",
    "\n",
    "print(len(random_ind_water),len(random_ind_land))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad8c21-d046-49b7-8bca-e4fb6cdf145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(2, 2,figsize=(20, 10))\n",
    "# var=0\n",
    "# for col in range(2):\n",
    "#     ax[col, 0].set_ylabel('Frequency') \n",
    "#     for row in range(2):\n",
    "#         variable=X_cpu.columns[var]\n",
    "#         if 'ndvi' in variable: \n",
    "#             continue\n",
    "#             var_bins = bin_boundaries\n",
    "#             log_values = False\n",
    "#         else: \n",
    "#             var_bins = None\n",
    "#             log_values = True\n",
    "#         ax[row, col].hist(\n",
    "#             [   \n",
    "#             X_cpu[variable][not_same_point.index].values\n",
    "#             ],\n",
    "#             label=[\n",
    "#             \"data\"\n",
    "#             ],\n",
    "#             bins=var_bins,\n",
    "#         color=['brown'], log=log_values) \n",
    "#         ax[row, col].set_xlabel(f'{variable}')\n",
    "#         var+=1\n",
    "#     ax[0,0].legend(loc='upper right',fontsize=20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f855d30f-505c-45cb-b02b-f94e5d38da4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d307387-dc6d-49ce-be7d-af4346607953",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Total random dataset used for training random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ccec67-bee6-4314-b013-ca09089b09ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster_land_random = X_land.iloc[random_ind_land]\n",
    "y_cluster_land_random = y_land.iloc[random_ind_land]\n",
    "X_cluster_water_random = X_water.iloc[random_ind_water]\n",
    "y_cluster_water_random = y_water.iloc[random_ind_water]\n",
    "\n",
    "X_cluster_random = pd.concat([X_cluster_land_random,X_cluster_water_random])\n",
    "y_cluster_random = pd.concat([y_cluster_land_random,y_cluster_water_random])\n",
    "\n",
    "#Combine the data so that we can shuffle the indices and keep the data together that should be\n",
    "All_data_random = pd.concat([X_cluster_random,y_cluster_random],axis=1).sample(frac=1)\n",
    "\n",
    "X_cluster_rfa_random = All_data_random[X_cluster_random.columns]\n",
    "y_cluster_rfa_random = All_data_random['water']\n",
    "\n",
    "print(X_cluster_rfa_random)\n",
    "print(y_cluster_rfa_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27677834-c156-4527-9f1d-1e53b33a7803",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_ind_land = np.random.choice(\n",
    "    np.arange(len(X_land)),len(random_ind_land),replace=False)\n",
    "print(random_ind_land)\n",
    "print(match_ind_land)\n",
    "\n",
    "match_ind_water = np.random.choice(\n",
    "    np.arange(len(X_water)),len(random_ind_water),replace=False)\n",
    "print(len(random_ind_water))\n",
    "print(len(match_ind_water))\n",
    "\n",
    "# X_match_land_random = X_land.iloc[match_ind_land]\n",
    "# y_match_land_random = y_land.iloc[match_ind_land]\n",
    "# X_match_water_random = X_water.iloc[match_ind_water]\n",
    "# y_match_water_random = y_water.iloc[match_ind_water]\n",
    "\n",
    "X_match_land_random = X_land.iloc[random_ind_land]\n",
    "y_match_land_random = y_land.iloc[random_ind_land]\n",
    "X_match_water_random = X_water.iloc[random_ind_water]\n",
    "y_match_water_random = y_water.iloc[random_ind_water]\n",
    "\n",
    "X_match_random = pd.concat([X_match_land_random,X_match_water_random])\n",
    "y_match_random = pd.concat([y_match_land_random,y_match_water_random])\n",
    "\n",
    "#Combine the data so that we can shuffle the indices and keep the data together that should be\n",
    "All_data_match_random = pd.concat([X_match_random,y_match_random],axis=1).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X_match_rfa_random = All_data_match_random[X_match_random.columns]\n",
    "y_match_rfa_random = All_data_match_random['water']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba98450-3b2d-46b7-8445-94e67036f79e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(All_data_random)\n",
    "print(X_match_rfa_random)\n",
    "print(y_match_rfa_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce60e95-e471-4c4a-a292-7d3e4152cca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8be6ccf-6e47-4297-bb0b-92277c4fc231",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Plotting paramater space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4907add-7b1c-4060-90c6-f971c49cc64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kme_land_random =  KMeans(n_clusters=CLUSTER_NUM, **common_params).fit(X_land)\n",
    "kmeans_output_land_random = kme_land_random.predict(X_land)\n",
    "kme_water_random = KMeans(n_clusters=CLUSTER_NUM, **common_params).fit(X_water)\n",
    "kmeans_output_water_random = kme_water_random.predict(X_water)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf76286d-7e58-444e-97ef-27cd5fa83dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(42)\n",
    "random_ind_land_eb = np.array([])\n",
    "random_ind_water_eb = []\n",
    "\n",
    "for cluster in np.unique(kmeans_output_water_random):\n",
    "    print(f'cluster {cluster}')\n",
    "    cluster_ind_water = np.where(kmeans_output_water_random == cluster)[0]\n",
    "    random_pts_water = np.random.choice(cluster_ind_water,COUNT,replace=False)\n",
    "    max_X_random_water = np.nanmax(X_water['sur_refl_b01_1'].iloc[random_pts_water])\n",
    "    if max_X_random_water < 10000:\n",
    "        random_ind_water_eb = np.append(random_ind_water_eb, random_pts_water)\n",
    "    else: print(f'Cluster {cluster} contains outliers')\n",
    "    \n",
    "    cluster_ind_land = np.where(kmeans_output_land_random == cluster)[0]\n",
    "    random_pts_land = np.random.choice(cluster_ind_land,COUNT,replace=False)\n",
    "    random_ind_land_eb = np.append(random_ind_land_eb, random_pts_land)\n",
    "    \n",
    "random_ind_water_eb = random_ind_water_eb.astype('int')\n",
    "random_ind_land_eb = random_ind_land_eb.astype('int')\n",
    "\n",
    "print(random_ind_water_eb,random_ind_land_eb)\n",
    "\n",
    "\n",
    "#############\n",
    "\n",
    "match_ind_land_eb = np.random.choice(np.arange(len(X_land)),len(random_ind_land_eb),replace=False)\n",
    "match_ind_water_eb = np.random.choice(np.arange(len(X_water)),len(random_ind_water_eb),replace=False)\n",
    "\n",
    "X_match_land_eb = X_land.iloc[match_ind_land_eb]\n",
    "X_match_water_eb = X_water.iloc[match_ind_water_eb]\n",
    "X_match_eb = pd.concat([X_match_land_eb,X_match_water_eb])\n",
    "\n",
    "X_cluster_land_eb = X_land.iloc[random_ind_land_eb]\n",
    "X_cluster_water_eb = X_water.iloc[random_ind_water_eb]\n",
    "X_cluster_eb = pd.concat([X_cluster_land_eb,X_cluster_water_eb])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ac0359-2a9c-44c9-b68b-dff80958e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "#############\n",
    "np.random.seed(42)\n",
    "random_ind_land_p = np.array([])\n",
    "random_ind_water_p= []\n",
    "\n",
    "for cluster in np.unique(kmeans_output_water_random):\n",
    "    print(f'cluster {cluster}')\n",
    "    cluster_ind_water = np.where(kmeans_output_water_random == cluster)[0]\n",
    "    # cluster_ind_water = np.where(bgm_water == cluster)[0]\n",
    "    COUNT_RANDOM_PULL_WATER = int(PERCENT_RANDOM_PULL*len(cluster_ind_water))\n",
    "    random_pts_water = np.random.choice(cluster_ind_water,COUNT_RANDOM_PULL_WATER,replace=False)\n",
    "    max_X_random_water = np.nanmax(X_water['sur_refl_b01_1'].iloc[random_pts_water])\n",
    "    if max_X_random_water < 10000:\n",
    "        random_ind_water_p = np.append(random_ind_water_p, random_pts_water)\n",
    "    else: print(f'Cluster {cluster} contains outliers')\n",
    "    \n",
    "    cluster_ind_land = np.where(kmeans_output_land_random == cluster)[0]\n",
    "    # cluster_ind_land = np.where(bgm_land == cluster)[0]\n",
    "    COUNT_RANDOM_PULL_LAND = int(PERCENT_RANDOM_PULL*len(cluster_ind_land))\n",
    "    random_pts_land = np.random.choice(cluster_ind_land,COUNT_RANDOM_PULL_LAND,replace=False)\n",
    "    random_ind_land_p = np.append(random_ind_land_p, random_pts_land)\n",
    "    print(f'Pulling {COUNT_RANDOM_PULL_WATER} Water pts and {COUNT_RANDOM_PULL_LAND} Land pts')\n",
    "    print()\n",
    "random_ind_water_p = random_ind_water_p.astype('int')\n",
    "random_ind_land_p = random_ind_land_p.astype('int')\n",
    "\n",
    "print(random_ind_water_p,random_ind_land_p)\n",
    "\n",
    "#############\n",
    "\n",
    "match_ind_land_p = np.random.choice(np.arange(len(X_land)),len(random_ind_land_p),replace=False)\n",
    "match_ind_water_p = np.random.choice(np.arange(len(X_water)),len(random_ind_water_p),replace=False)\n",
    "\n",
    "X_match_land_p = X_land.iloc[match_ind_land_p]\n",
    "X_match_water_p = X_water.iloc[match_ind_water_p]\n",
    "X_match_p = pd.concat([X_match_land_p,X_match_water_p])\n",
    "\n",
    "X_cluster_land_p = X_land.iloc[random_ind_land_p]\n",
    "X_cluster_water_p = X_water.iloc[random_ind_water_p]\n",
    "X_cluster_p = pd.concat([X_cluster_land_p,X_cluster_water_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0da7981-4061-4f85-8c3b-5aef1a65fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2,figsize=(20, 10))\n",
    "var=0\n",
    "for col in range(2):\n",
    "    ax[col, 0].set_ylabel('Frequency') \n",
    "    for row in range(2):\n",
    "        variable=X_land.columns[var]\n",
    "        if 'ndvi' in variable: \n",
    "            # var_bins = bin_boundaries\n",
    "            log_values = False\n",
    "        else: \n",
    "            # var_bins = None\n",
    "            log_values = True\n",
    "        ax[row, col].hist(\n",
    "            [  \n",
    "            # X_cluster_eb[variable].values,\n",
    "            # X_match_eb[variable].values,\n",
    "            X_cluster_p[variable].values,\n",
    "            X_match_p[variable].values,\n",
    "            ],\n",
    "            label=[\n",
    "            # f\"EB Cluster {len(X_cluster_eb)}\",\n",
    "            # \"EB Match\",\n",
    "            f\"P Cluster {len(X_match_p)}\",\n",
    "            f\"P Match\"\n",
    "            ],\n",
    "            #bins=var_bins,\n",
    "        #color=['darkgreen','lightgreen','darkblue','lightblue'], log=log_values) \n",
    "        color=['plum','darkorchid'], log=log_values) \n",
    "        ax[row, col].set_xlabel(f'{variable}')\n",
    "        var+=1\n",
    "    ax[0,0].legend(loc='upper right',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b82dde-f9a2-40c5-a21a-c76837a96d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =  plt.subplots(1, 1,figsize=(10, 5))\n",
    "variable = X_land.columns[0]\n",
    "\n",
    "plt.hist(\n",
    "    [X_cluster_p[variable].values,\n",
    "     X_match_p[variable].values,\n",
    "    ],\n",
    "    label=[\n",
    "        f\"P Cluster {len(X_match_p)}\",\n",
    "        f\"P Match\"\n",
    "        ],\n",
    "    color=['plum','darkorchid'], log=True) \n",
    "\n",
    "plt.ylabel('Frequency') \n",
    "plt.xlabel(f'{variable}')\n",
    "plt.legend(loc='upper right',fontsize=20)   \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed9581-542c-433e-a139-1a45a83d9612",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3244e03-63f5-45f2-b28d-dfcd6b7bc8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_rf_objective(trial):\n",
    "    list_trees = [75, 100, 125, 150, 175, 200, 250, 300, 400, 500]\n",
    "    max_depth = [5, 10, 30, 50, 80, 90, 100, 110]\n",
    "    min_samples_leaf = [1, 2, 3, 4, 5]\n",
    "    min_samples_split = [2, 4, 8, 10]\n",
    "    bootstrap = [True, False]\n",
    "    max_features = ['auto', 'sqrt', 'log2']\n",
    "    \n",
    "    param = {'n_estimators': trial.suggest_categorical('n_estimators', list_trees), \n",
    "       'max_depth':trial.suggest_categorical('max_depth', max_depth), \n",
    "       'min_samples_split':trial.suggest_categorical('min_samples_split', min_samples_split), \n",
    "       'min_samples_leaf':trial.suggest_categorical('min_samples_leaf', min_samples_leaf), \n",
    "       'bootstrap': trial.suggest_categorical('bootstrap', bootstrap),\n",
    "       'criterion':'gini', \n",
    "       #'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 1e-8, 1.0, log=True), \n",
    "       'max_features':trial.suggest_categorical('max_features', max_features), \n",
    "       'max_leaf_nodes':None, \n",
    "       'min_impurity_decrease':0.0, \n",
    "       'oob_score':False, \n",
    "       'n_jobs':-1, \n",
    "       # 'random_state':42, \n",
    "       'verbose':0, \n",
    "       'warm_start':False, \n",
    "       'class_weight':None, \n",
    "       'ccp_alpha':0.0, \n",
    "       'max_samples':None\n",
    "        }\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    cv_scores = np.empty(5)\n",
    "    for idx, (train_idx, val_idx) in enumerate(cv.split(X,y)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = skRF(**param)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_val)\n",
    "        cv_scores[idx] = f1_score(y_val, preds)\n",
    "        if cv_scores[idx] == 0.0:\n",
    "            print('Pruning because of 0.0 score.')\n",
    "            return 0.0\n",
    "        print('Fold {}: {}'.format(idx, cv_scores[idx]))\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "search_space={\n",
    "    \"n_estimators\": [75, 100, 125, 150, 175, 200, 250, 300, 400, 500],\n",
    "    \"max_depth\" : [5,10, 30, 50, 80, 90, 100, 110],\n",
    "    \"min_samples_leaf\" : [1, 2, 3, 4, 5],\n",
    "    \"min_samples_split\" : [2, 4, 8, 10],\n",
    "    \"bootstrap\" : [True, False],\n",
    "    \"max_features\" : ['auto', 'sqrt', 'log2'],\n",
    "    \n",
    "}\n",
    "TREES_AND_DEPTH_ONLY = False\n",
    "GRID_SEARCH = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7b08f3e-44b5-48f8-acb8-33d538cae39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_rf_objective(trial):\n",
    "    list_trees = [75, 100, 125, 150, 175, 200, 250, 300, 400, 500]\n",
    "    max_depth = [5, 10, 30, 50, 80, 90, 100, 110]\n",
    "    min_samples_leaf = [1, 2, 3, 4, 5]\n",
    "    min_samples_split = [2, 4, 8, 10]\n",
    "    bootstrap = [True, False]\n",
    "    max_features = ['auto', 'sqrt', 'log2']\n",
    "    \n",
    "    param = {'n_estimators': trial.suggest_categorical('n_estimators', list_trees), \n",
    "        'max_depth':trial.suggest_categorical('max_depth', max_depth), \n",
    "        'min_samples_split':trial.suggest_categorical('min_samples_split', min_samples_split), \n",
    "        'min_samples_leaf':trial.suggest_categorical('min_samples_leaf', min_samples_leaf), \n",
    "        'max_features':trial.suggest_categorical('max_features', max_features), \n",
    "            }\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    #######################\n",
    "    # HERE IS WHERE TO CHANGE THE X,Y DATASET USED FOR TRAINING\n",
    "    #######################\n",
    "   \n",
    "    cv_scores = np.empty(5)\n",
    "    for idx, (train_idx, val_idx) in enumerate(cv.split(X.to_pandas(),y.to_pandas())):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model = cuRFC(**param)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_val)\n",
    "        cv_scores[idx] = f1_score(y_val.to_numpy(), preds.to_numpy())\n",
    "        del model, preds\n",
    "        if cv_scores[idx] == 0.0:\n",
    "            print('Pruning because of 0.0 score.')\n",
    "            return 0.0\n",
    "        print('Fold {}: {}'.format(idx, cv_scores[idx]))\n",
    "    return np.mean(cv_scores)\n",
    "    \n",
    "search_space={\n",
    "    \"n_estimators\": [75, 100, 125, 150, 175, 200, 250, 300, 400, 500],\n",
    "    \"max_depth\" : [5,10, 30, 50, 80, 90, 100, 110],\n",
    "    \"min_samples_leaf\" : [1, 2, 3, 4, 5],\n",
    "    \"min_samples_split\" : [2, 4, 8, 10],\n",
    "    \"bootstrap\" : [True, False],\n",
    "    \"max_features\" : ['auto', 'sqrt', 'log2'],\n",
    "    \n",
    "}\n",
    "TREES_AND_DEPTH_ONLY = False\n",
    "GRID_SEARCH = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca239ea1-6081-466a-9458-c158eb7f2393",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-11-09 15:44:18,593]\u001b[0m A new study created in memory with name: RF Tuning Grid Search\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: 0.9666724776970128\n",
      "Fold 1: 0.9673792653507108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-11-09 15:44:38,598]\u001b[0m Trial 0 failed with parameters: {'n_estimators': 250, 'max_depth': 80, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_3273859/3230189718.py\", line 27, in gpu_rf_objective\n",
      "    model.fit(X_train, y_train)\n",
      "  File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/contextlib.py\", line 75, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/cuml/internals/api_decorators.py\", line 190, in wrapper\n",
      "    ret = func(*args, **kwargs)\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-11-09 15:44:38,603]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:19\u001b[0m\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/optuna/study/study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[13], line 27\u001b[0m, in \u001b[0;36mgpu_rf_objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     24\u001b[0m y_train, y_val \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39miloc[train_idx], y\u001b[38;5;241m.\u001b[39miloc[val_idx]\n\u001b[1;32m     26\u001b[0m model \u001b[38;5;241m=\u001b[39m cuRFC(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparam)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[1;32m     29\u001b[0m cv_scores[idx] \u001b[38;5;241m=\u001b[39m f1_score(y_val\u001b[38;5;241m.\u001b[39mto_numpy(), preds\u001b[38;5;241m.\u001b[39mto_numpy())\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/cuml/internals/api_decorators.py:190\u001b[0m, in \u001b[0;36m_make_decorator_function.<locals>.decorator_function.<locals>.decorator_closure.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m     set_api_output_dtype(output_dtype)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process_return:\n\u001b[0;32m--> 190\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "if GRID_SEARCH:\n",
    "    study = optuna.create_study(study_name='RF Tuning Grid Search', \n",
    "                                direction='maximize',\n",
    "                                sampler=optuna.samplers.GridSampler(search_space))\n",
    "    \n",
    "else:\n",
    "    study = optuna.create_study(study_name='RF Tuning',\n",
    "                                direction='maximize')\n",
    "#Objective is under the functions area\n",
    "\n",
    "#####################################################################\n",
    "#CHANGE HERE FOR DIFFERENT MODELING TYPE\n",
    "#rf_objective or xgb_objective\n",
    "#####################################################################\n",
    "if GPU is False:\n",
    "    study.optimize(cpu_rf_objective, n_trials=25, timeout=30*600)\n",
    "else: \n",
    "    study.optimize(gpu_rf_objective, n_trials=25, timeout=30*600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef706118-c253-40b8-b067-3adba3bdffd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Training and output best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6602bb0b-7735-4127-a350-a554dcb8a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = study.best_trials            \n",
    "max_trial_score = max([trial.values[0] for trial in trials])\n",
    "max_trial_params = [trial.params for trial in trials \n",
    "                        if trial.values[0] == max_trial_score][0]\n",
    "max_trial_params['n_jobs'] = -1\n",
    "score_print = int(np.round(max_trial_score,4)*1000)\n",
    "print(max_trial_score)\n",
    "print(score_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb27e0f-3ab7-41f8-bda7-2c0e7564578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = max_trial_params\n",
    "hyperparameters['n_jobs'] = -1\n",
    "print('Using these params:')\n",
    "print(hyperparameters)\n",
    "tuned_classifier = skRF(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dea7c9-7927-4f2c-9b11-8bfa0dd082ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "tuned_classifier.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29964f62-43bb-453c-9b65-a7526af71bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # save the model to disk\n",
    "# filename = f'rfa_models/MODIS_RFA_Targeted_v000_MaxScore{score_print}_sfcref127ndvi.pkl'\n",
    "# print(filename)\n",
    "# pickle.dump(tuned_classifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3220fba-351e-4bc5-9e24-f2af5dcf901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickled_model = pickle.load(open('rfa_models/MODIS_RFA_v201_EBCluster_sfcref127ndvi_4.pkl', 'rb'))\n",
    "# print(pickled_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee12a19-8cf8-472d-ba4d-e269e7356272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ILAB Kernel (TensorFlow)",
   "language": "python",
   "name": "tensorflow-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
