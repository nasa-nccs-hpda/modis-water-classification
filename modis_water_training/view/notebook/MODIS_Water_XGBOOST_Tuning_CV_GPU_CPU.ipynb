{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODIS Water XGBOOST Hyperparameter Tuning - Cross Validation\n",
    "\n",
    "Version: 0.4.0\n",
    "\n",
    "Date modified: 04.21.2022\n",
    "\n",
    "Modified by: Caleb Spradlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import optuna\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, precision_score\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "#GDAL Stuff\n",
    "from osgeo import gdalconst\n",
    "from osgeo import gdal\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURE_OUTPUT_DIR = '/explore/nobackup/projects/ilab/scratch/cssprad1/MODIS_water/code/tmp'\n",
    "RASTER_OUTPUT_DIR = '/explore/nobackup/projects/ilab/scratch/cssprad1/MODIS_water/code/tmp'\n",
    "MODEL_OUTPUT_DIR = '/explore/nobackup/projects/ilab/scratch/cssprad1/MODIS_water/models/'\n",
    "\n",
    "training_data_basepath = '/explore/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/training_data/v4.1.0'\n",
    "\n",
    "qaMaskPath = '/explore/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/qa'\n",
    "waterMaskPath = '/explore/nobackup/people/mcarrol2/MODIS_water/v5_outputs'\n",
    "\n",
    "test_data_basepath = '/explore/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/test_data/'\n",
    "\n",
    "GPU = True\n",
    "TILE = 'Golden'\n",
    "MODEL = 'xgboost'\n",
    "TEST_RATIO = 0.2\n",
    "RANDOM_STATE = 42\n",
    "LABEL_NAME = 'water'\n",
    "DATA_TYPE = np.int16\n",
    "# Columns that are offset, years, julian days, etc (always need to be dropped).\n",
    "offsets_indexes = ['x_offset', 'y_offset', 'year', 'julian_day',] #'tileID']\n",
    "# Columns that the user wants to drop for training purposes. \n",
    "colsToDrop = []#'sur_refl_b03_1','sur_refl_b04_1','sur_refl_b05_1','ndwi1','ndwi2']\n",
    "colsToDropTraining = colsToDrop.copy()\n",
    "colsToDropTraining.extend(offsets_indexes)\n",
    "v_names = ['sur_refl_b01_1','sur_refl_b02_1','sur_refl_b03_1',\n",
    "           'sur_refl_b04_1','sur_refl_b05_1','sur_refl_b06_1',\n",
    "           'sur_refl_b07_1','ndvi','ndwi1','ndwi2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colsToDrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x_offset', 'y_offset', 'year', 'julian_day']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colsToDropTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fpath, colsToDrop, yCol='water', testSize=0.2, randomState=42, \n",
    "              dataType=np.float32, cpu=True, splitXY=False, trainTestSplit=False,\n",
    "             applyLog=False, imbalance=False, frac=0.1, land=False, multi=False, \n",
    "              multisample=1000000):\n",
    "    \"\"\"\n",
    "    Simple helper function for loading data to be used by models\n",
    "    :param fpath: Path to the data to be ingested.\n",
    "    :param dataType: Data type to convert ingested data to.\n",
    "    :param colsToDrop: Columns which are not necessary, from which to drop.\n",
    "    :param testSize: Ration to\n",
    "    \"\"\"\n",
    "    if multi:\n",
    "        all_dfs = [pd.read_csv(path_) for path_ in fpath]\n",
    "        df = pd.concat(all_dfs).sample(n=multisample, random_state=randomState)\n",
    "        print('DF length: {}'.format(len(df.index)))\n",
    "    else:   \n",
    "        df = pd.read_parquet(fpath) if '.parquet' in fpath else pd.read_csv(fpath)\n",
    "    df = df[df['sur_refl_b01_1'] + df['sur_refl_b02_1'] != 0]\n",
    "    df = df[df['sur_refl_b07_1'] + df['sur_refl_b02_1'] != 0]\n",
    "    df = df[df['sur_refl_b06_1'] + df['sur_refl_b02_1'] != 0]\n",
    "    df = df.drop(columns=colsToDrop)\n",
    "    cleanedDF = df[~df.isin([np.NaN, np.inf, -np.inf]).any(1)].dropna(axis=0).astype(dataType)\n",
    "    if applyLog:\n",
    "        for col in cleanedDF.drop([yCol], axis=1).columns:\n",
    "            print('Applying log1p func to {}'.format(col))\n",
    "            cleanedDF[col] = np.log1p(cleanedDF[col])\n",
    "        cleanedDF = cleanedDF[~cleanedDF.isin([np.NaN, np.inf, -np.inf]).any(1)].dropna(axis=0)\n",
    "    df = None\n",
    "    if imbalance:\n",
    "        groupedDF = cleanedDF.groupby('water')\n",
    "        dfs = [groupedDF.get_group(y) for y in groupedDF.groups]\n",
    "        sampledDF = dfs[1].sample(frac=frac)if land else dfs[0].sample(frac=frac)\n",
    "        concatDF = sampledDF.append(dfs[0]) if land else sampledDF.append(dfs[1])\n",
    "        concatDF = concatDF.sample(frac=1)\n",
    "        concatDF = concatDF.reset_index()\n",
    "        cleanedDF = concatDF.drop(columns=['index'])\n",
    "    if not splitXY:\n",
    "        return cleanedDF\n",
    "    X = cleanedDF.drop([yCol], axis=1).astype(dataType)\n",
    "    y = cleanedDF[yCol].astype(dataType)\n",
    "    if trainTestSplit:\n",
    "        return train_test_split(X, y, test_size=TEST_RATIO)\n",
    "    else:\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "- Read in to cuDF Dataframe\n",
    "- Drop unnecessary columns\n",
    "- Split into Xs and Ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/explore/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/training_data/v4.1.0/MOD09_Golden_Masked_5500000_4_1_0.parquet.gzip',\n",
      " '/explore/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/training_data/v4.1.0/MOD09_Golden_Masked_865249_Water_Imbalance_4_2_0.parquet.gzip',\n",
      " '/explore/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/training_data/v4.1.0/MOD09_Golden_Masked_957000_4_2_0.parquet.gzip']\n",
      "/explore/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/training_data/v4.1.0/MOD09_Golden_Masked_5500000_4_1_0.parquet.gzip\n"
     ]
    }
   ],
   "source": [
    "glob_string = os.path.join(training_data_basepath,'MOD*{}*.parquet.gzip'.format(TILE))\n",
    "data_paths = sorted([fv for fv in glob.glob(glob_string)])\n",
    "data_path = data_paths[0]\n",
    "pprint(data_paths)\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data(fpath=data_path,\n",
    "                 colsToDrop=colsToDropTraining,\n",
    "                 dataType=DATA_TYPE,\n",
    "                 cpu=True,\n",
    "                 splitXY=True,\n",
    "                 imbalance=False,\n",
    "                 trainTestSplit=False,\n",
    "                 multi=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sur_refl_b01_1\n",
      "sur_refl_b02_1\n",
      "sur_refl_b03_1\n",
      "sur_refl_b04_1\n",
      "sur_refl_b05_1\n",
      "sur_refl_b06_1\n",
      "sur_refl_b07_1\n",
      "ndvi\n",
      "ndwi1\n",
      "ndwi2\n"
     ]
    }
   ],
   "source": [
    "_ = [print(column) for column in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sur_refl_b01_1</th>\n",
       "      <td>5471295.0</td>\n",
       "      <td>841.851125</td>\n",
       "      <td>990.548719</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>644.0</td>\n",
       "      <td>1073.0</td>\n",
       "      <td>13141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sur_refl_b02_1</th>\n",
       "      <td>5471295.0</td>\n",
       "      <td>1652.850555</td>\n",
       "      <td>1324.888768</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>1877.0</td>\n",
       "      <td>2569.0</td>\n",
       "      <td>12205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sur_refl_b03_1</th>\n",
       "      <td>5471295.0</td>\n",
       "      <td>512.695311</td>\n",
       "      <td>695.314373</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>9119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sur_refl_b04_1</th>\n",
       "      <td>5471295.0</td>\n",
       "      <td>736.332239</td>\n",
       "      <td>799.820567</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>612.0</td>\n",
       "      <td>878.0</td>\n",
       "      <td>9550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sur_refl_b05_1</th>\n",
       "      <td>5471295.0</td>\n",
       "      <td>1890.367700</td>\n",
       "      <td>1472.162995</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>2263.0</td>\n",
       "      <td>3108.0</td>\n",
       "      <td>9383.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sur_refl_b06_1</th>\n",
       "      <td>5471295.0</td>\n",
       "      <td>1638.655989</td>\n",
       "      <td>1387.571289</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1591.0</td>\n",
       "      <td>2660.0</td>\n",
       "      <td>7855.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sur_refl_b07_1</th>\n",
       "      <td>5471295.0</td>\n",
       "      <td>1081.531180</td>\n",
       "      <td>1091.516029</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>808.0</td>\n",
       "      <td>1705.0</td>\n",
       "      <td>9357.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ndvi</th>\n",
       "      <td>5471295.0</td>\n",
       "      <td>459.291045</td>\n",
       "      <td>6800.464754</td>\n",
       "      <td>-32762.0</td>\n",
       "      <td>-1809.0</td>\n",
       "      <td>2494.0</td>\n",
       "      <td>5008.0</td>\n",
       "      <td>32727.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ndwi1</th>\n",
       "      <td>5471295.0</td>\n",
       "      <td>-2057.844288</td>\n",
       "      <td>5551.530698</td>\n",
       "      <td>-32758.0</td>\n",
       "      <td>-2864.0</td>\n",
       "      <td>-873.0</td>\n",
       "      <td>767.0</td>\n",
       "      <td>32762.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ndwi2</th>\n",
       "      <td>5471295.0</td>\n",
       "      <td>-33.837911</td>\n",
       "      <td>7037.874811</td>\n",
       "      <td>-32758.0</td>\n",
       "      <td>-1266.0</td>\n",
       "      <td>950.0</td>\n",
       "      <td>4178.0</td>\n",
       "      <td>32762.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count         mean          std      min     25%     50%  \\\n",
       "sur_refl_b01_1  5471295.0   841.851125   990.548719   -100.0   162.0   644.0   \n",
       "sur_refl_b02_1  5471295.0  1652.850555  1324.888768   -100.0   169.0  1877.0   \n",
       "sur_refl_b03_1  5471295.0   512.695311   695.314373   -100.0   178.0   343.0   \n",
       "sur_refl_b04_1  5471295.0   736.332239   799.820567   -100.0   274.0   612.0   \n",
       "sur_refl_b05_1  5471295.0  1890.367700  1472.162995   -100.0   146.0  2263.0   \n",
       "sur_refl_b06_1  5471295.0  1638.655989  1387.571289   -100.0   160.0  1591.0   \n",
       "sur_refl_b07_1  5471295.0  1081.531180  1091.516029   -100.0    71.0   808.0   \n",
       "ndvi            5471295.0   459.291045  6800.464754 -32762.0 -1809.0  2494.0   \n",
       "ndwi1           5471295.0 -2057.844288  5551.530698 -32758.0 -2864.0  -873.0   \n",
       "ndwi2           5471295.0   -33.837911  7037.874811 -32758.0 -1266.0   950.0   \n",
       "\n",
       "                   75%      max  \n",
       "sur_refl_b01_1  1073.0  13141.0  \n",
       "sur_refl_b02_1  2569.0  12205.0  \n",
       "sur_refl_b03_1   571.0   9119.0  \n",
       "sur_refl_b04_1   878.0   9550.0  \n",
       "sur_refl_b05_1  3108.0   9383.0  \n",
       "sur_refl_b06_1  2660.0   7855.0  \n",
       "sur_refl_b07_1  1705.0   9357.0  \n",
       "ndvi            5008.0  32727.0  \n",
       "ndwi1            767.0  32762.0  \n",
       "ndwi2           4178.0  32762.0  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_interesting_idx(df, column, threshold, greaterThan=True):\n",
    "    dfToReturn = df[df[column] > threshold] if \\\n",
    "        greaterThan else df[df[column] < threshold]\n",
    "    return dfToReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sur_refl_b01_1</th>\n",
       "      <th>sur_refl_b02_1</th>\n",
       "      <th>sur_refl_b03_1</th>\n",
       "      <th>sur_refl_b04_1</th>\n",
       "      <th>sur_refl_b05_1</th>\n",
       "      <th>sur_refl_b06_1</th>\n",
       "      <th>sur_refl_b07_1</th>\n",
       "      <th>ndvi</th>\n",
       "      <th>ndwi1</th>\n",
       "      <th>ndwi2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>-3</td>\n",
       "      <td>128</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>25536</td>\n",
       "      <td>-13529</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-36</td>\n",
       "      <td>20</td>\n",
       "      <td>-66</td>\n",
       "      <td>-55</td>\n",
       "      <td>15</td>\n",
       "      <td>33</td>\n",
       "      <td>25</td>\n",
       "      <td>30536</td>\n",
       "      <td>-2452</td>\n",
       "      <td>-1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>8</td>\n",
       "      <td>-24</td>\n",
       "      <td>179</td>\n",
       "      <td>136</td>\n",
       "      <td>10</td>\n",
       "      <td>469</td>\n",
       "      <td>365</td>\n",
       "      <td>20000</td>\n",
       "      <td>-11078</td>\n",
       "      <td>-11407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>15</td>\n",
       "      <td>-9</td>\n",
       "      <td>178</td>\n",
       "      <td>121</td>\n",
       "      <td>-10</td>\n",
       "      <td>332</td>\n",
       "      <td>246</td>\n",
       "      <td>25536</td>\n",
       "      <td>-10557</td>\n",
       "      <td>-10759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>4</td>\n",
       "      <td>-22</td>\n",
       "      <td>89</td>\n",
       "      <td>125</td>\n",
       "      <td>37</td>\n",
       "      <td>84</td>\n",
       "      <td>90</td>\n",
       "      <td>14444</td>\n",
       "      <td>-17096</td>\n",
       "      <td>-16470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5499346</th>\n",
       "      <td>15</td>\n",
       "      <td>-10</td>\n",
       "      <td>96</td>\n",
       "      <td>68</td>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>25</td>\n",
       "      <td>15536</td>\n",
       "      <td>-18695</td>\n",
       "      <td>-23333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5499633</th>\n",
       "      <td>12</td>\n",
       "      <td>-10</td>\n",
       "      <td>204</td>\n",
       "      <td>107</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>25</td>\n",
       "      <td>21072</td>\n",
       "      <td>-17407</td>\n",
       "      <td>-23333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5499688</th>\n",
       "      <td>1</td>\n",
       "      <td>-6</td>\n",
       "      <td>87</td>\n",
       "      <td>50</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>14000</td>\n",
       "      <td>-15536</td>\n",
       "      <td>-4464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5499706</th>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "      <td>143</td>\n",
       "      <td>74</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>15536</td>\n",
       "      <td>10000</td>\n",
       "      <td>3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5499872</th>\n",
       "      <td>8</td>\n",
       "      <td>-5</td>\n",
       "      <td>140</td>\n",
       "      <td>77</td>\n",
       "      <td>-10</td>\n",
       "      <td>15</td>\n",
       "      <td>-2</td>\n",
       "      <td>22203</td>\n",
       "      <td>-20000</td>\n",
       "      <td>4285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38458 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sur_refl_b01_1  sur_refl_b02_1  sur_refl_b03_1  sur_refl_b04_1  \\\n",
       "23                    5              -3             128              67   \n",
       "24                  -36              20             -66             -55   \n",
       "174                   8             -24             179             136   \n",
       "392                  15              -9             178             121   \n",
       "467                   4             -22              89             125   \n",
       "...                 ...             ...             ...             ...   \n",
       "5499346              15             -10              96              68   \n",
       "5499633              12             -10             204             107   \n",
       "5499688               1              -6              87              50   \n",
       "5499706               3              -2             143              74   \n",
       "5499872               8              -5             140              77   \n",
       "\n",
       "         sur_refl_b05_1  sur_refl_b06_1  sur_refl_b07_1   ndvi  ndwi1  ndwi2  \n",
       "23                    7              20               1  25536 -13529  20000  \n",
       "24                   15              33              25  30536  -2452  -1111  \n",
       "174                  10             469             365  20000 -11078 -11407  \n",
       "392                 -10             332             246  25536 -10557 -10759  \n",
       "467                  37              84              90  14444 -17096 -16470  \n",
       "...                 ...             ...             ...    ...    ...    ...  \n",
       "5499346               7              33              25  15536 -18695 -23333  \n",
       "5499633               9              37              25  21072 -17407 -23333  \n",
       "5499688              15               4               8  14000 -15536  -4464  \n",
       "5499706              18               0              -1  15536  10000   3333  \n",
       "5499872             -10              15              -2  22203 -20000   4285  \n",
       "\n",
       "[38458 rows x 10 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndviOverTenK = output_interesting_idx(X, 'ndvi', 10000)\n",
    "ndviOverTenK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose which combination of hyperparameters to train:\n",
    "\n",
    "- n_estimators: Number of learners for the xgboost to fit.\n",
    "- booster: Which booster to use\n",
    "- lamda: L2 regularization rate\n",
    "- alpha: L1 regularization rate\n",
    "- subsample: sampling ration for training data\n",
    "- colsample_bytree: sampling according to each tree\n",
    "- max_depth: maximum depth of the tree, signifies complexity\n",
    "- min_child_weight: minimum child weight, the larger the term, the more complex\n",
    "- gamme: how selective the algorithm is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_precision(predt: np.ndarray, dtrain: xgb.DMatrix):\n",
    "    y = dtrain.get_label()\n",
    "    tresh_func = np.vectorize(lambda x: 1 if x > 0.5 else 0)\n",
    "    pred_y = tresh_func(predt)\n",
    "    precision = precision_score(y, pred_y)\n",
    "    return 'clf_precision', precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    hyperparameters = {\n",
    "        \"verbosity\": 0,\n",
    "        'disable_default_eval_metric': 1,\n",
    "        #\"learning_rate\": 0.001,\n",
    "        #\"objective\": \"binary:logistic\",\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"hist\",\n",
    "        #\"booster\":\"dart\",\n",
    "        #\"rate_drop\":0.9,\n",
    "        \"gpu_id\": 0,\n",
    "        \"n_jobs\": -1,\n",
    "        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [25, 50, 100, 250, 200, 250, 300, 350, 375, 400]),\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.1, 0.5),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 0.5),\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        \"max_depth\":  trial.suggest_int(\"max_depth\", 3, 15, step=1), \n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        \"min_child_weight\":  trial.suggest_int(\"min_child_weight\", 2, 7), \n",
    "        \"eta\":  trial.suggest_float(\"eta\", 1e-8, 1.0, log=True), \n",
    "        # defines how selective algorithm is.\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True), \n",
    "        \"grow_policy\":  trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
    "    }\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation_0-auc\")\n",
    "    training_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "    xgb_cv = xgb.cv(dtrain=training_dmatrix, \n",
    "                    params=hyperparameters, \n",
    "                    nfold=10, \n",
    "                    early_stopping_rounds=10,\n",
    "                    as_pandas=True,\n",
    "                    feval=xgb_precision)\n",
    "    test_clf_precision = xgb_cv['test-clf_precision-mean'].values\n",
    "    print('Cross Validation History: Precision')\n",
    "    for i, val in enumerate(test_clf_precision):\n",
    "        print('Fold {}: {}'.format(i, val))\n",
    "    return np.mean(test_clf_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the search space\n",
    "\n",
    "Set the search space for the hyperparameter tuning to search through once. This needs to be in conjunction with the `trial.suggest...` part of the hyperparameters dictionary defined aboce. \n",
    "\n",
    "Ex.\n",
    "\n",
    "If `max_depth` is defined in `hyperparameters` in the `objective` function above as such\n",
    "\n",
    "`\"max_depth\":  trial.suggest_int(\"max_depth\", 10, 30, step=5),` \n",
    "\n",
    "then we need to pair that with the limited search space below as such:\n",
    "\n",
    "`search_space={\"max_depth\":[10, 15, 20, 25, 30]}`\n",
    "\n",
    "This is not the most ideal way of ensuring that trials are not repeated, however it works for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space={\n",
    "    \"max_depth\":[5, 7, 10, 15],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set number of trials and timeout\n",
    "\n",
    "Set the number of trials `n_trials=<num of desired trials>` and the maximum time for the total study `timeout=<timeout in minutes>*60`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 16:31:44,968]\u001b[0m A new study created in memory with name: xgboost hyperparameter tuning\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "study = optuna.create_study(study_name='xgboost hyperparameter tuning', \n",
    "                            direction='maximize',\n",
    "                           )#sampler=optuna.samplers.GridSampler(search_space))\n",
    "study.optimize(objective, n_trials=10, timeout=10*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "trials = study.best_trials\n",
    "trial_score = max([trial.values[0] for trial in trials])\n",
    "best_trial_params = [trial.params for trial in trials if trial.values[0] == trial_score][0]\n",
    "print(best_trial_params)\n",
    "print(trial_score)\n",
    "\n",
    "trial_scores = [trial.values for trial in trials]\n",
    "trial_params = [trial.params for trial in trials]\n",
    "\n",
    "for score, param in zip(trial_scores, trial_params):\n",
    "    print(score)\n",
    "    for k, v in param.items():\n",
    "        print(\"     {}: {}\".format(k, v))\n",
    "\n",
    "study_df = study.trials_dataframe()\n",
    "study_df.to_csv(\"hyperopt_tuning_trial_{}_xgboost.csv\".format(\n",
    "    datetime.datetime.now().strftime('%Y_%m_%d_%H_%M')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.matplotlib.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    optuna.visualization.matplotlib.plot_param_importances(study)\n",
    "except:\n",
    "    print('Tuning only one hyper-parameter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train model given the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = best_trial_params\n",
    "\n",
    "base_params = {\"verbosity\": 0,\n",
    "               \"objective\": \"binary:logistic\",\n",
    "               # \"tree_method\": \"hist\",\n",
    "               \"n_jobs\": -1}\n",
    "\n",
    "hyperparameters.update(base_params)\n",
    "print('Using these params:')\n",
    "pprint(hyperparameters)\n",
    "classifier = xgb.XGBClassifier(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, shuffle=True)\n",
    "eval_set = [(X_train, y_train), (X_val, y_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, \n",
    "               y_train,\n",
    "               eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "               eval_metric=['error', 'auc'],\n",
    "               early_stopping_rounds=20,\n",
    "               verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = classifier.evals_result()\n",
    "epochs = len(results['validation_0']['error'])\n",
    "x_axis = range(0, epochs)\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "ax.plot(x_axis, results['validation_0']['auc'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['auc'], label='Test')\n",
    "ax.legend()\n",
    "plt.ylabel('AUC')\n",
    "plt.title('XGBoost AUC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = classifier.evals_result()\n",
    "epochs = len(results['validation_0']['error'])\n",
    "x_axis = range(0, epochs)\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "ax.legend()\n",
    "plt.ylabel('error')\n",
    "plt.title('XGBoost error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing and training/testing data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = classifier.predict(X_train)\n",
    "test_predictions = classifier.predict(X_val)\n",
    "prediction_probs = classifier.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = test_predictions.astype(np.int16)\n",
    "y_test_int = y_val.astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Performance')\n",
    "print('-------------------------------------------------------')\n",
    "print(classification_report(y_val, test_predictions))\n",
    "# From docs: tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_int, test_predictions).ravel()\n",
    "recall = (tp / (tp + fp))\n",
    "print('Test Recall')\n",
    "print('-------------------------------------------------------')\n",
    "print(recall)\n",
    "print('\\nTest Matthews Correlation Coefficient (MCC)')\n",
    "print('-------------------------------------------------------')\n",
    "mcc = matthews_corrcoef(y_test_int, test_predictions)\n",
    "print(mcc)\n",
    "print('\\nConfusion Matrix')\n",
    "print('-------------------------------------------------------')\n",
    "print('TP: {:9} FN: {:9}'.format(tp, fn))\n",
    "print('FP: {:9} TN: {:9}'.format(fp, tn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_importance_results = permutation_importance(classifier,\n",
    "                                                        X=X_val,\n",
    "                                                        y=y_val,\n",
    "                                                        n_repeats=5,\n",
    "                                                        random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png_save_path = 'mw_{}_{}_{}_{}_permutation_importance.png'.format(\n",
    "    TILE,\n",
    "    hyperparameters['n_estimators'],\n",
    "    MODEL,\n",
    "    datetime.datetime.now().strftime('%Y_%m_%d_%H_%M'))\n",
    "\n",
    "png_save_path = os.path.join(FIGURE_OUTPUT_DIR, png_save_path)\n",
    "print('Saved to: {}'.format(png_save_path))\n",
    "sorted_idx = permutation_importance_results.importances_mean.argsort()\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.barh(X_train.columns[sorted_idx], permutation_importance_results.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "# plt.savefig(png_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'mw_{}_{}_{}_4.1.0_tuned_{}.sav'.format(TILE,\n",
    "                                                      hyperparameters['n_estimators'],\n",
    "                                                      MODEL,\n",
    "                                                      'gpu' if GPU else 'cpu',\n",
    "                                                      datetime.datetime.now().strftime('%Y_%m_%d_%H_%M'))\n",
    "\n",
    "model_save_path = os.path.join(MODEL_OUTPUT_DIR, model_save_path)\n",
    "print('Saving model to: {}'.format(model_save_path))\n",
    "print(classifier)\n",
    "joblib.dump(classifier, model_save_path, compress=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing: Raster testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../..')\n",
    "from modis_water_training.model.TabularModisDataGenerator import TabularModisDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE = 'h11v10'\n",
    "DAY = 201\n",
    "YEAR = 2006\n",
    "PATH = os.path.join(test_data_basepath, '{}/'.format(TILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabularGen = TabularModisDataGenerator(tile=TILE, year=YEAR, julianDays=[DAY])\n",
    "sensorDir = f'/css/modis/Collection6.1/L2G/MOD09GA/{YEAR}'\n",
    "modisFilesDict = tabularGen._readFiles(julianDay=DAY, sensorDir=sensorDir)\n",
    "modisFilesList = list(modisFilesDict.values())\n",
    "qa_mask, qa_mask_path = tabularGen._generateBadDataMask(day=DAY, files=modisFilesDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_list_gq = [fn for fn in modisFilesList\n",
    "            if 'sur_refl' in fn and 'GQ' in fn]\n",
    "vars_list_gq.sort()\n",
    "\n",
    "vars_list_ga = [fn for fn in modisFilesList\n",
    "            if 'sur_refl' in fn and 'GQ' not in fn]\n",
    "vars_list_ga.sort()\n",
    "\n",
    "vars_list = vars_list_gq\n",
    "vars_list.extend(vars_list_ga)\n",
    "\n",
    "vars_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readRasterToArray(vars_list):\n",
    "    vrt_options = gdal.BuildVRTOptions(xRes=231.656358, yRes=231.656358, separate=True)\n",
    "    dd = gdal.BuildVRT('tmp.vrt', vars_list, options=vrt_options)\n",
    "    nrows, ncols = dd.RasterYSize, dd.RasterXSize\n",
    "    newshp = (ncols*nrows, dd.RasterCount+3)\n",
    "    img = np.empty(newshp, dtype=np.int16)\n",
    "    for b in range(len(vars_list)):\n",
    "        img[:, b] = dd.GetRasterBand(b+1).ReadAsArray().astype(np.int16).ravel()\n",
    "    dd = None\n",
    "    img[:, len(vars_list)] = ((img[:, 1] - img[:, 0]) / (img[:, 1] + img[:, 0])) * 10000\n",
    "    img[:, len(vars_list)+1] = ((img[:, 1] - img[:, 5]) / (img[:, 1] + img[:, 5])) * 10000\n",
    "    img[:, len(vars_list)+2] = ((img[:, 1] - img[:, 6]) / (img[:, 1] + img[:, 6])) * 10000\n",
    "    if os.path.exists('tmp.vrt'):\n",
    "        os.remove('tmp.vrt')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = readRasterToArray(vars_list)\n",
    "print('Raster as ndarray')\n",
    "print(im)\n",
    "print('{} MB size'.format((im.size * im.itemsize) / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRaster(img_chunk, colsToDrop=None):\n",
    "    \"\"\"\n",
    "    Function given a raster in the form of a nxn matrix, will\n",
    "    convert the matrix to a GPU/CPU-bound data frame then perform \n",
    "    predictions given the loaded model.\n",
    "    \n",
    "    Return the prediction matrix, the prediction probabilities\n",
    "    for each and the dataframe converted to host.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(img_chunk, columns=v_names, dtype=np.int16)\n",
    "    df = df.drop(columns=colsToDrop) if colsToDrop else df\n",
    "    print('Making predictions from raster')\n",
    "    predictions = classifier.predict(df).astype(np.int16)\n",
    "    predictionsProbs = classifier.predict_proba(df).astype(np.float32)\n",
    "    return predictions, predictionsProbs, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictedRaster, predictedProbaRaster, df = predictRaster(im, colsToDrop=colsToDrop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputreshapet raster: description and histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape the unravelled matrix back to the 4800x4800 raster shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp = (4800, 4800)\n",
    "matrix = np.asarray(predictedRaster)\n",
    "reshp = matrix.reshape(shp)\n",
    "reshp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the QA Mask and the Water Mask for the h09v05 TILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_list = [fn for fn in glob.glob(os.path.join(waterMaskPath, '{}'.format(YEAR), '*{}*{}_v5.tif'.format(TILE, YEAR)))]\n",
    "print(os.path.join(waterMaskPath, '{}'.format(YEAR), '*{}*.tif'.format(TILE)))\n",
    "water_mask_path = water_list[0]\n",
    "print(water_mask_path)\n",
    "water_mask = gdal.Open(water_mask_path, gdal.GA_ReadOnly)\n",
    "waterMaskMatrix = water_mask.GetRasterBand(1).ReadAsArray().astype(np.int16)\n",
    "waterMask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterMaskMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask out results if QA Mask says pixel is \"bad\"\n",
    "Mask out water mask if QA Mask says pixel is \"bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskedResult = np.where(qa_mask == 0, reshp, 255)\n",
    "waterMasked = np.where(qa_mask == 0, waterMaskMatrix, 255)\n",
    "waterMaskRavel = waterMasked.ravel()\n",
    "imWater = (waterMaskRavel == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating stats for predicted and truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = np.where((waterMasked == 1) & (maskedResult == 1), 1, 0)\n",
    "tn = np.where((waterMasked == 0) & (maskedResult == 0), 1, 0)\n",
    "fp = np.where((waterMasked == 0) & (maskedResult == 1), 1, 0)\n",
    "fn = np.where((waterMasked == 1) & (maskedResult == 0), 1, 0)\n",
    "total = np.count_nonzero(waterMasked == 1) + np.count_nonzero(waterMasked == 0)\n",
    "truePositives = np.count_nonzero(tp == 1)\n",
    "trueNegatives = np.count_nonzero(tn == 1)\n",
    "falsePositives = np.count_nonzero(fp == 1)\n",
    "falseNegatives = np.count_nonzero(fn == 1)\n",
    "accuracy = (truePositives + trueNegatives) / (truePositives + trueNegatives + falsePositives + falseNegatives)\n",
    "jians = truePositives / (truePositives + trueNegatives)\n",
    "pc = truePositives / (truePositives + falsePositives)\n",
    "rc = truePositives / (truePositives + falseNegatives)\n",
    "f1 = truePositives / (truePositives + (0.5*(falsePositives + falseNegatives)))\n",
    "mcc_denom_nosqrt = (truePositives+falsePositives)*(truePositives+falseNegatives)*(trueNegatives+falsePositives)*(trueNegatives+falseNegatives)\n",
    "mcc_numerator = (truePositives*trueNegatives) - (falsePositives*falseNegatives)\n",
    "mcc = mcc_numerator/math.sqrt(mcc_denom_nosqrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count num of occurences for each class with the masked predicted result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countNoData = np.count_nonzero(maskedResult == 255)\n",
    "countLand = np.count_nonzero(maskedResult == 0)\n",
    "countWater = np.count_nonzero(maskedResult == 1)\n",
    "print('Predicted\\n Nodata occurences: {}\\n Land occurance: {}\\n Water occurances: {}'.format(countNoData, countLand, countWater))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count num of occurences for each class with the water mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countNoDataT = np.count_nonzero(waterMasked == 255)\n",
    "countLandT = np.count_nonzero(waterMasked == 0)\n",
    "countWaterT = np.count_nonzero(waterMasked == 1)\n",
    "print('Truth Vals\\n Nodata occurences: {}\\n Land occurance: {}\\n Water occurances: {}'.format(countNoDataT, countLandT, countWaterT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model metrics on raster data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Metrics of Accuracy for Raster Test Data')\n",
    "print('True Positives:  {}'.format(truePositives))\n",
    "print('True Negatives:  {}'.format(trueNegatives))\n",
    "print('False Positives: {}'.format(falsePositives))\n",
    "print('False Negatives: {}'.format(falseNegatives))\n",
    "print('Total \"good\" data: {}'.format(total))\n",
    "print('Accuracy*: {}'.format(accuracy))\n",
    "print('Precision: {}'.format(pc))\n",
    "print('Recall: {}'.format(rc))\n",
    "print('f1: {}'.format(f1))\n",
    "print('MCC: {}'.format(mcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output predicted raster to GeoTiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outPath = os.path.join(RASTER_OUTPUT_DIR, '{}_{}_{}_predicted_{}.tif'.format(YEAR, DAY, TILE, MODEL))\n",
    "waterMaskForDay = os.path.join(RASTER_OUTPUT_DIR, 'waterMask_{}_qa_{}.tif'.format(YEAR, DAY, TILE, MODEL))\n",
    "outPathProba = os.path.join(RASTER_OUTPUT_DIR, '{}_{}_{}_predicted_probabilities_{}.tif'.format(YEAR, DAY, TILE, MODEL))\n",
    "print(outPath)\n",
    "print(waterMaskForDay)\n",
    "print(outPathProba)\n",
    "\n",
    "ds = gdal.Open(vars_list[0], gdal.GA_ReadOnly)\n",
    "geo = ds.GetGeoTransform()\n",
    "proj = ds.GetProjection()\n",
    "ncols = ds.RasterXSize\n",
    "nrows = ds.RasterYSize\n",
    "print('Transform')\n",
    "print(geo)\n",
    "print('Projection')\n",
    "print(proj)\n",
    "print('Width')\n",
    "print(ncols)\n",
    "print('Height')\n",
    "print(nrows)\n",
    "ds = None\n",
    "\n",
    "# Output predicted binary raster masked with good-bad mask.\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "outDs = driver.Create(outPath, ncols, nrows, 1, gdal.GDT_Float32, options=['COMPRESS=LZW'])\n",
    "outDs.SetGeoTransform(geo)\n",
    "outDs.SetProjection(proj)\n",
    "outBand = outDs.GetRasterBand(1)\n",
    "outBand.WriteArray(maskedResult)\n",
    "outBand.SetNoDataValue(255)\n",
    "outDs.FlushCache()\n",
    "outDs = None\n",
    "outBand = None\n",
    "driver = None\n",
    "\n",
    "# Output water mask with good-bad masked.\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "outDs = driver.Create(waterMaskForDay, ncols, nrows, 1, gdal.GDT_Int16, options=['COMPRESS=LZW'])\n",
    "outDs.SetGeoTransform(geo)\n",
    "outDs.SetProjection(proj)\n",
    "outBand = outDs.GetRasterBand(1)\n",
    "outBand.WriteArray(waterMasked)\n",
    "outBand.SetNoDataValue(255)\n",
    "outDs.FlushCache()\n",
    "outDs = None\n",
    "outBand = None\n",
    "driver = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folium viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import rasterio as rio\n",
    "import tempfile\n",
    "\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from pyproj import Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Uses rasterio to open a raster, get the metadata and crs\n",
    "# associated with it and get all the subdatasets in the file.\n",
    "# This is very useful for hdf files such as MODIS hdfs.\n",
    "# -----------------------------------------------------------------------------\n",
    "def print_subdatasets(filename):\n",
    "    bands_to_return = []\n",
    "    with rio.open(filename) as dataset:\n",
    "        meta_data = dataset.meta\n",
    "        crs = dataset.read_crs()\n",
    "        \n",
    "        print([name for name in dataset.subdatasets if search_term in name])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Gets a tiff that has the correct metadata for that tile, gets the metadata\n",
    "# from the source tif and copies to a destination tiff. \n",
    "# -----------------------------------------------------------------------------     \n",
    "def add_metadata_to_annual_product(filepath, model_type, year, tile):\n",
    "    metadata_pull_src = [fv for fv in glob.glob(os.path.join(filepath, \"{}-1*-{}-MOD-*.tif\".format(year, tile)))][0]\n",
    "    with rio.open(metadata_pull_src) as src:\n",
    "        src_meta = src.meta\n",
    "    dst_tiffs = [os.path.join(filepath, fn) for fn in os.listdir(filepath) if \"{0}-{1}\".format(year, tile) in os.path.basename(fn)]\n",
    "    [copy_meta(dst_tiff, src_meta, metadata_pull_src) for dst_tiff in dst_tiffs]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Given a path to a tiff with no metadata, assign the metadata given to that\n",
    "# tiff.\n",
    "# -----------------------------------------------------------------------------     \n",
    "def copy_meta(dst_path, src_meta, src_name):\n",
    "    print('Copying metadata from {} to {}'.format(src_name, dst_path))\n",
    "    with rio.open(dst_path, 'r+') as dst:\n",
    "        dst.crs = src_meta['crs']\n",
    "        dst.transform = src_meta['transform']        \n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Given a tiff file as input, open the tiff and get the transform needed to\n",
    "# reproject from the tiff's source crs to the one we want (EPSG:3857).\n",
    "# For each band in the tiff, open then reproject it into the desired crs\n",
    "# then write to a temporary file. Return the path to the temp file.\n",
    "# -----------------------------------------------------------------------------\n",
    "def reproject_to_3857(input_tiff):\n",
    "    # Set desitnation CRS\n",
    "    dst_crs = f\"EPSG:3857\"\n",
    "\n",
    "    # set out path\n",
    "    out_path_rproj = os.path.join(tempfile.gettempdir(), input_tiff.split('/')[-1].replace('.tif','-3857.tif'))\n",
    "\n",
    "    with rio.open(input_tiff) as src:\n",
    "        # get src bounds and transform\n",
    "        transform, width, height = calculate_default_transform(src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
    "        print('Transform: {}'.format(transform))\n",
    "        print('Width: {} Height: {}'.format(width, height))\n",
    "        kwargs = src.meta.copy()\n",
    "        kwargs.update({'crs': dst_crs,\n",
    "                   'transform': transform,\n",
    "                   'width': width,\n",
    "                   'height': height})\n",
    "    \n",
    "        # reproject and write to file\n",
    "        with rio.open(out_path_rproj, 'w', **kwargs) as dst:\n",
    "            for i in range(1, src.count + 1):\n",
    "                reproject(source=rio.band(src, i),\n",
    "                      destination=rio.band(dst, i),\n",
    "                      src_transform=src.transform,\n",
    "                      src_crs=src.crs,\n",
    "                      dst_transform=transform,\n",
    "                      dst_crs=dst_crs,\n",
    "                      resampling=Resampling.nearest)\n",
    "    return out_path_rproj\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# In order for folium to work properly we need to pass it the bounding box\n",
    "# of the tiff in the form of lat and lon. This is done by using rasterio.\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_bounds(tiff_3857):\n",
    "    with rio.open(tiff_3857) as src:\n",
    "        src_crs = src.crs['init'].upper()\n",
    "        min_lon, min_lat, max_lon, max_lat = src.bounds\n",
    "    bounds_orig = [[min_lat, min_lon], [max_lat, max_lon]]\n",
    "    bounds = []\n",
    "    dst_crs = 'EPSG:4326'\n",
    "    for item in bounds_orig:   \n",
    "        #converting to lat/lon\n",
    "        lat = item[0]\n",
    "        lon = item[1]\n",
    "        proj = Transformer.from_crs(int(src_crs.split(\":\")[1]), int(dst_crs.split(\":\")[1]), always_xy=True)\n",
    "        lon_n, lat_n = proj.transform(lon, lat)\n",
    "        bounds.append([lat_n, lon_n])\n",
    "    center_lon = bounds[0][1] + (bounds[1][1] - bounds[0][1])/2\n",
    "    center_lat = bounds[0][0] + (bounds[1][0] - bounds[0][0])/2\n",
    "    return {'bounds': bounds, 'center': (center_lon, center_lat)}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Use rasterio to open and read in the desired band name as a nd-array.\n",
    "# -----------------------------------------------------------------------------\n",
    "def open_and_get_band(file_name, band_num=1):\n",
    "    with rio.open(file_name) as data:\n",
    "        b = data.read(band_num)\n",
    "    return b\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Given an nd-array (band) and the bounds in lat lon of the nd-array, return\n",
    "# a folium layer. To add on the map.\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_overlay(band, meta_dict, name, opacity=1.0, show=True):\n",
    "    return folium.raster_layers.ImageOverlay(band, \n",
    "                                             bounds=meta_dict['bounds'], \n",
    "                                             name=name, \n",
    "                                             opacity=opacity, \n",
    "                                             show=show)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# We don't need to keep those temp files we made for the reprojections around.\n",
    "# -----------------------------------------------------------------------------\n",
    "def cleanup(filename):\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "    else:\n",
    "        print('No file: {} exists.'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_3857 = reproject_to_3857(outPath)\n",
    "mod44_3857 = reproject_to_3857(water_mask_path)\n",
    "qa_3857 = reproject_to_3857(qa_mask_path)\n",
    "\n",
    "mask_d = get_bounds(mask_3857)\n",
    "mod44_d = get_bounds(mod44_3857)\n",
    "qa_d = get_bounds(qa_3857)\n",
    "\n",
    "mask_b1 = open_and_get_band(mask_3857, 1)\n",
    "mask_b1 = np.where(mask_b1 == 255, 0, mask_b1)\n",
    "mod44_b1 = open_and_get_band(mod44_3857, 1)\n",
    "mod44_b1 = np.where(mod44_b1 == 255, 0, mod44_b1)\n",
    "qa_b1 = open_and_get_band(qa_3857, 1)\n",
    "\n",
    "\n",
    "cleanup(mask_3857)\n",
    "cleanup(mod44_3857)\n",
    "cleanup(qa_3857)\n",
    "\n",
    "zeros = np.zeros_like(mask_b1)\n",
    "mask_rgba = np.dstack((mask_b1, zeros, zeros, mask_b1))\n",
    "mod44_rgba = np.dstack((zeros, mod44_b1, zeros, mod44_b1))\n",
    "qa_rgba = np.dstack((qa_b1, qa_b1, qa_b1, qa_b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[mask_d['center'][1], mask_d['center'][0]],\n",
    "                   tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}', zoom_start = 6, attr='Google', control_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.add_child(get_overlay(mask_rgba, mask_d, '{}-{}-{} model water mask'.format(YEAR, DAY, TILE), opacity=0.8))\n",
    "m.add_child(get_overlay(mod44_rgba, mod44_d, '{} {} MOD44W mask'.format(TILE, YEAR), opacity=0.8, show=False))\n",
    "m.add_child(get_overlay(qa_rgba, qa_d, '{}-{}-{} MW QA mask'.format(YEAR, DAY, TILE), opacity=0.8, show=False))\n",
    "m.add_child(plugins.MousePosition())\n",
    "m.add_child(plugins.MeasureControl())\n",
    "m.add_child(folium.LayerControl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ILAB Kernel",
   "language": "python",
   "name": "ilab-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
