{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODIS Water Random Forest Hyperparameter Tuning\n",
    "\n",
    "Version: 0.7.0\n",
    "\n",
    "Date modified: 07.24.2023\n",
    "\n",
    "Modified by: Caleb Spradlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path   \n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.ensemble import RandomForestClassifier as skRF\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, precision_score, f1_score\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, matthews_corrcoef\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# GPU-based frameworks\n",
    "\n",
    "import cudf\n",
    "import cupy as cp\n",
    "\n",
    "from cuml.ensemble import RandomForestClassifier as cuRFC\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "\n",
    "#GDAL Stuff\n",
    "from osgeo import gdalconst\n",
    "from osgeo import gdal\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURE_OUTPUT_DIR = '/explore/nobackup/projects/ilab/scratch/cssprad1/MODIS_water/code/tmp'\n",
    "RASTER_OUTPUT_DIR = '/explore/nobackup/projects/ilab/scratch/cssprad1/MODIS_water/code/tmp'\n",
    "MODEL_OUTPUT_DIR = '/explore/nobackup/projects/ilab/scratch/cssprad1/MODIS_water/models/'\n",
    "\n",
    "training_data_basepath = '/explore/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/training_data/v2.0.1'\n",
    "\n",
    "qaMaskPath = '/explore/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/qa'\n",
    "waterMaskPath = '/explore/nobackup/people/mcarrol2/MODIS_water/v5_outputs'\n",
    "\n",
    "test_data_basepath = '/explore/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/test_data/'\n",
    "\n",
    "GPU = True\n",
    "TILE = 'GLOBAL'\n",
    "MODEL = 'rf'\n",
    "TEST_RATIO = 0.2\n",
    "RANDOM_STATE = 42\n",
    "LABEL_NAME = 'water'\n",
    "DATA_TYPE = np.int16\n",
    "# Columns that are offset, years, julian days, etc (always need to be dropped).\n",
    "offsets_indexes = ['x_offset', 'y_offset', 'year', 'julian_day', ]#'tileID']\n",
    "# Columns that the user wants to drop for training purposes. \n",
    "colsToDrop = [] # ['sur_refl_b03_1','sur_refl_b04_1','sur_refl_b05_1','ndwi1','ndwi2']\n",
    "colsToDropTraining = colsToDrop.copy()\n",
    "colsToDropTraining.extend(offsets_indexes)\n",
    "v_names = ['sur_refl_b01_1','sur_refl_b02_1','sur_refl_b03_1',\n",
    "           'sur_refl_b04_1','sur_refl_b05_1','sur_refl_b06_1',\n",
    "           'sur_refl_b07_1','ndvi','ndwi1','ndwi2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsToDrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsToDropTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fpath, colsToDrop, yCol='water', testSize=0.2, randomState=42, \n",
    "              dataType=np.float32, cpu=False, splitXY=False, trainTestSplit=False,\n",
    "             applyLog=False, imbalance=False, frac=0.1, land=False, multi=False, \n",
    "              multisample=1000000):\n",
    "    \"\"\"\n",
    "    Simple helper function for loading data to be used by models\n",
    "    :param fpath: Path to the data to be ingested.\n",
    "    :param dataType: Data type to convert ingested data to.\n",
    "    :param colsToDrop: Columns which are not necessary, from which to drop.\n",
    "    :param testSize: Ration to\n",
    "    \"\"\"\n",
    "    if multi:\n",
    "        all_dfs = [pd.read_csv(path_) for path_ in fpath]\n",
    "        df = pd.concat(all_dfs).sample(n=multisample, random_state=randomState)\n",
    "        print('DF length: {}'.format(len(df.index)))\n",
    "    else:   \n",
    "        df = pd.read_parquet(fpath) if '.parquet' in fpath else pd.read_csv(fpath)\n",
    "    df = df[df['sur_refl_b01_1'] + df['sur_refl_b02_1'] != 0]\n",
    "    df = df[df['sur_refl_b07_1'] + df['sur_refl_b02_1'] != 0]\n",
    "    df = df[df['sur_refl_b06_1'] + df['sur_refl_b02_1'] != 0]\n",
    "    df = df.drop(columns=colsToDrop)\n",
    "    cleanedDF = df[~df.isin([np.NaN, np.inf, -np.inf]).any(1)].dropna(axis=0).astype(dataType)\n",
    "    cleanedDF = cudf.from_pandas(cleanedDF) if not cpu else cleanedDF\n",
    "    if applyLog:\n",
    "        for col in cleanedDF.drop([yCol], axis=1).columns:\n",
    "            print('Applying log1p func to {}'.format(col))\n",
    "            cleanedDF[col] = np.log1p(cleanedDF[col])\n",
    "        cleanedDF = cleanedDF[~cleanedDF.isin([np.NaN, np.inf, -np.inf]).any(1)].dropna(axis=0)\n",
    "    df = None\n",
    "    if imbalance:\n",
    "        if land:\n",
    "            print('Imbalancing data, sampling {} from water'.format(frac))\n",
    "        else:\n",
    "            print('Imbalancing data, sampling {} from land'.format(frac))\n",
    "        groupedDF = cleanedDF.groupby('water')\n",
    "        dfs = [groupedDF.get_group(y) for y in groupedDF.groups]\n",
    "        sampledDF = dfs[1].sample(frac=frac)if land else dfs[0].sample(frac=frac)\n",
    "        concatDF = sampledDF.append(dfs[0]) if land else sampledDF.append(dfs[1])\n",
    "        concatDF = concatDF.sample(frac=1)\n",
    "        concatDF = concatDF.reset_index()\n",
    "        cleanedDF = concatDF.drop(columns=['index'])\n",
    "    if not splitXY:\n",
    "        return cleanedDF\n",
    "    X = cleanedDF.drop([yCol], axis=1).astype(dataType)\n",
    "    y = cleanedDF[yCol].astype(dataType)\n",
    "    if trainTestSplit:\n",
    "        return train_test_split(X, y, test_size=TEST_RATIO)\n",
    "    else:\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "- Read in to cuDF or pandas Dataframe\n",
    "- Drop unnecessary columns\n",
    "- Clean data\n",
    "- Split into Xs and Ys\n",
    "- Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_string = os.path.join(training_data_basepath,'MOD*{}*.parquet.gzip'.format(TILE))\n",
    "data_paths = sorted([fv for fv in glob.glob(glob_string)])\n",
    "data_path = data_paths[0]\n",
    "pprint(data_paths)\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, y, y_test = load_data(fpath=data_path,\n",
    "                                             colsToDrop=colsToDropTraining,\n",
    "                                             dataType=cp.float32,\n",
    "                                             cpu=False,\n",
    "                                             splitXY=True,\n",
    "                                             imbalance=False,\n",
    "                                             trainTestSplit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [print(column) for column in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_interesting_idx(df, column, threshold, greaterThan=True):\n",
    "    dfToReturn = df[df[column] > threshold] if \\\n",
    "        greaterThan else df[df[column] < threshold]\n",
    "    return dfToReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndviOverTenK = output_interesting_idx(X, 'ndvi', 10000)\n",
    "ndviOverTenK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_interesting_idx(X, 'ndwi1', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_interesting_idx(X, 'ndwi2', 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = cuRFC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set TREES_AND_DEPTH_ONLY to True if you only want to tune number of estimators and max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TREES_AND_DEPTH_ONLY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    list_trees = [75, 100, 125, 150, 175, 200, 250, 300, 400, 500]\n",
    "    max_depth = [16, 30, 50, 80, 90, 100, 110]\n",
    "    min_samples_leaf = [1, 2, 3, 4, 5]\n",
    "    min_samples_split = [2, 4, 8, 10]\n",
    "    bootstrap = [True, False]\n",
    "    max_features = ['auto', 'sqrt', 'log2']\n",
    "    \n",
    "    if TREES_AND_DEPTH_ONLY:\n",
    "        param = {'n_estimators': trial.suggest_categorical('n_estimators', list_trees), \n",
    "                 'max_depth':trial.suggest_categorical('max_depth', max_depth), \n",
    "                 'random_state':42,\n",
    "                }\n",
    "    else:\n",
    "        param = {'n_estimators': trial.suggest_categorical('n_estimators', list_trees), \n",
    "                       'max_depth':trial.suggest_categorical('max_depth', max_depth), \n",
    "                       'min_samples_split':trial.suggest_categorical('min_samples_split', min_samples_split), \n",
    "                       'min_samples_leaf':trial.suggest_categorical('min_samples_leaf', min_samples_leaf), \n",
    "                       'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 1e-8, 1.0, log=True), \n",
    "                       'max_features':trial.suggest_categorical('max_features', max_features), \n",
    "                       'random_state':42, \n",
    "                      }\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    cv_scores = np.empty(5)\n",
    "    for idx, (train_idx, val_idx) in enumerate(cv.split(X.to_pandas(), y.to_pandas())):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = cuRFC(**param)\n",
    "        model.fit(X_train, \n",
    "                  y_train)\n",
    "        preds = model.predict(X_val)\n",
    "        cv_scores[idx] = f1_score(y_val.to_numpy(), preds.to_numpy())\n",
    "        del model, preds\n",
    "        if cv_scores[idx] == 0.0:\n",
    "            print('Pruning because of 0.0 score.')\n",
    "            return 0.0\n",
    "        print('Fold {}: {}'.format(idx, cv_scores[idx]))\n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start hyperparameter tuning trials.\n",
    "\n",
    "Ex output: Trial 0 finished with values: [0.9527224325581133, 0.9667626330841518, 9684.0] and parameters:\n",
    "\n",
    "The metrics in order are: [precision, f1, false positives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space={\n",
    "    \"n_estimators\": [75, 100, 125, 150, 175, 200, 250, 300, 400, 500],\n",
    "    \"max_depth\" : [30, 50, 80, 90, 100, 110],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to only exhaust the search space above, set `GRID_SEARCH = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SEARCH = False # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "if GRID_SEARCH:\n",
    "    study = optuna.create_study(study_name='RF Tuning Grid Search', \n",
    "                                direction='maximize',\n",
    "                                sampler=optuna.samplers.GridSampler(search_space))\n",
    "    \n",
    "else:\n",
    "    study = optuna.create_study(study_name='RF Tuning',\n",
    "                                direction='maximize')\n",
    "study.optimize(objective, n_trials=20, timeout=30*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get metrics from the overall hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "trials = study.best_trials\n",
    "trial_score = max([trial.values[0] for trial in trials])\n",
    "best_trial_params = [trial.params for trial in trials if trial.values[0] == trial_score][0]\n",
    "print(best_trial_params)\n",
    "print(trial_score)\n",
    "\n",
    "trial_scores = [trial.values for trial in trials]\n",
    "trial_params = [trial.params for trial in trials]\n",
    "\n",
    "for score, param in zip(trial_scores, trial_params):\n",
    "    print(score)\n",
    "    for k, v in param.items():\n",
    "        print(\"     {}: {}\".format(k, v))\n",
    "\n",
    "study_df = study.trials_dataframe()\n",
    "study_df.to_csv(\"hyperopt_tuning_trial_{}_rf.csv\".format(\n",
    "    datetime.datetime.now().strftime('%Y_%m_%d_%H_%M')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the optimization history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.matplotlib.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize hyperparameter importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    optuna.visualization.matplotlib.plot_param_importances(study)\n",
    "except:\n",
    "    print('Tuning only one hyper-parameter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with the best hyperparamers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = best_trial_params\n",
    "hyperparameters['n_jobs'] = -1\n",
    "print('Using these params:')\n",
    "pprint(hyperparameters)\n",
    "classifier = skRF(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X.to_pandas(), y.to_pandas()\n",
    "X_test, y_test = X_test.to_pandas(), y_test.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing and training/testing data validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = classifier.score(X_test, y_test)\n",
    "score = round(score, 3)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = classifier.predict(X)\n",
    "test_predictions = classifier.predict(X_test)\n",
    "prediction_probs = classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = test_predictions.astype(np.int16)\n",
    "y_test_int = y_test.astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Performance')\n",
    "print('-------------------------------------------------------')\n",
    "print(classification_report(y_test, test_predictions))\n",
    "# From docs: tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_int, test_predictions).ravel()\n",
    "recall = (tp / (tp + fp))\n",
    "print('Test Recall')\n",
    "print('-------------------------------------------------------')\n",
    "print(recall)\n",
    "print('\\nTest Matthews Correlation Coefficient (MCC)')\n",
    "print('-------------------------------------------------------')\n",
    "mcc = matthews_corrcoef(y_test_int, test_predictions)\n",
    "print(mcc)\n",
    "print('\\nConfusion Matrix')\n",
    "print('-------------------------------------------------------')\n",
    "print('TP: {:9} FN: {:9}'.format(tp, fn))\n",
    "print('FP: {:9} TN: {:9}'.format(fp, tn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NDVI Over 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndviOver10Predictions = classifier.predict(ndviOverTenK)\n",
    "withPrediction = ndviOverTenK.assign(water_prediction=ndviOver10Predictions)\n",
    "print('Number of rows: {}'.format(len(ndviOverTenK.index)))\n",
    "print('Number of water predictions: {}'.format(np.count_nonzero(ndviOver10Predictions == 1)))\n",
    "print('Number of land predictions: {}'.format(np.count_nonzero(ndviOver10Predictions == 0)))\n",
    "withPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withPrediction[withPrediction['water_prediction'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutaion importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_importance_results = permutation_importance(classifier,\n",
    "                                                        X=X_test,\n",
    "                                                        y=y_test,\n",
    "                                                        n_repeats=5,\n",
    "                                                        random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png_save_path = 'mw_{}_{}_{}_{}_{}_permutation_importance.png'.format(\n",
    "    TILE,\n",
    "    score,\n",
    "    MODEL,\n",
    "    hyperparameters['n_estimators'],\n",
    "    datetime.datetime.now().strftime('%Y_%m_%d_%H_%M'))\n",
    "\n",
    "png_save_path = os.path.join(FIGURE_OUTPUT_DIR, png_save_path)\n",
    "\n",
    "sorted_idx = permutation_importance_results.importances_mean.argsort()\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.barh(X_test.columns[sorted_idx], permutation_importance_results.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Permutation Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Garbage collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'mw_{}_{}_{}_{}_4.1.0_tuned_{}.sav'.format(TILE,\n",
    "                                                              score,\n",
    "                                                              hyperparameters['n_estimators'],\n",
    "                                                              MODEL,\n",
    "                                                              'cpu',\n",
    "                                                              datetime.datetime.now().strftime('%Y_%m_%d_%H_%M'))\n",
    "model_save_path = os.path.join(MODEL_OUTPUT_DIR, model_save_path)\n",
    "print('Saving model to: {}'.format(model_save_path))\n",
    "print(classifier)\n",
    "joblib.dump(classifier, model_save_path, compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier # joblib.load('/adapt/nobackup/projects/ilab/scratch/cssprad1/MODIS_water/models/mw_h11v10_0.983_500_rf_4.2.0_tuned_cpu.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing: raster testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../..')\n",
    "from modis_water_training.model.TabularModisDataGenerator import TabularModisDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE = 'h11v10'\n",
    "DAY = 201\n",
    "YEAR = 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabularGen = TabularModisDataGenerator(tile=TILE, year=YEAR, julianDays=[DAY])\n",
    "sensorDir = f'/css/modis/Collection6.1/L2G/MOD09GA/{YEAR}'\n",
    "modisFilesDict = tabularGen._readFiles(julianDay=DAY, sensorDir=sensorDir)\n",
    "modisFilesList = list(modisFilesDict.values())\n",
    "qa_mask, qa_mask_path = tabularGen._generateBadDataMask(day=DAY, files=modisFilesDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_list_gq = [fn for fn in modisFilesList\n",
    "            if 'sur_refl' in fn and 'GQ' in fn]\n",
    "vars_list_gq.sort()\n",
    "\n",
    "vars_list_ga = [fn for fn in modisFilesList\n",
    "            if 'sur_refl' in fn and 'GQ' not in fn]\n",
    "vars_list_ga.sort()\n",
    "\n",
    "vars_list = vars_list_gq\n",
    "vars_list.extend(vars_list_ga)\n",
    "\n",
    "vars_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data \n",
    "We don't need to slice because we have more than enough GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readRasterToArray(vars_list):\n",
    "    vrt_options = gdal.BuildVRTOptions(xRes=231.656358, yRes=231.656358, separate=True)\n",
    "    dd = gdal.BuildVRT('tmp.vrt', vars_list, options=vrt_options)\n",
    "    nrows, ncols = dd.RasterYSize, dd.RasterXSize\n",
    "    newshp = (ncols*nrows, dd.RasterCount+3)\n",
    "    img = np.empty(newshp, dtype=np.int16)\n",
    "    for b in range(len(vars_list)):\n",
    "        img[:, b] = dd.GetRasterBand(b+1).ReadAsArray().astype(np.int16).ravel()\n",
    "    dd = None\n",
    "    img[:, len(vars_list)] = ((img[:, 1] - img[:, 0]) / (img[:, 1] + img[:, 0])) * 10000\n",
    "    img[:, len(vars_list)+1] = ((img[:, 1] - img[:, 5]) / (img[:, 1] + img[:, 5])) * 10000\n",
    "    img[:, len(vars_list)+2] = ((img[:, 1] - img[:, 6]) / (img[:, 1] + img[:, 6])) * 10000\n",
    "    if os.path.exists('tmp.vrt'):\n",
    "        os.remove('tmp.vrt')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = readRasterToArray(vars_list)\n",
    "print('Raster as ndarray')\n",
    "print(im)\n",
    "print('{} MB size'.format((im.size * im.itemsize) / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRaster(img_chunk, colsToDrop=None):\n",
    "    \"\"\"\n",
    "    Function given a raster in the form of a nxn matrix, will\n",
    "    convert the matrix to a GPU/CPU-bound data frame then perform \n",
    "    predictions given the loaded model.\n",
    "    \n",
    "    Return the prediction matrix, the prediction probabilities\n",
    "    for each and the dataframe converted to host.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(img_chunk, columns=v_names, dtype=np.int16)\n",
    "    df = df.drop(columns=colsToDrop) if colsToDrop else df\n",
    "    print('Making predictions from raster')\n",
    "    predictions = classifier.predict(df).astype(np.int16)\n",
    "    predictionsProbs = classifier.predict_proba(df).astype(np.float32)\n",
    "    return predictions, predictionsProbs, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictedRaster, predictedProbaRaster, df = predictRaster(im, colsToDrop=colsToDrop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raster as a DataFrame: description and histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape the unravelled matrix back to the 4800x4800 raster shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp = (4800, 4800)\n",
    "matrix = np.asarray(predictedRaster)\n",
    "reshp = matrix.reshape(shp)\n",
    "reshp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the QA Mask and the Water Mask for the h09v05 TILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_list = [fn for fn in glob.glob(os.path.join(waterMaskPath, '{}'.format(YEAR), '*{}*{}_v5.tif'.format(TILE, YEAR)))]\n",
    "print(os.path.join(waterMaskPath, '{}'.format(YEAR), '*{}*.tif'.format(TILE)))\n",
    "water_mask_path = water_list[0]\n",
    "print(water_mask_path)\n",
    "water_mask = gdal.Open(water_mask_path, gdal.GA_ReadOnly)\n",
    "waterMaskMatrix = water_mask.GetRasterBand(1).ReadAsArray().astype(np.int16)\n",
    "waterMask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterMaskMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask out results if QA Mask says pixel is \"bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskedResult = np.where(qa_mask == 0, reshp, 255)\n",
    "waterMasked = np.where(qa_mask == 0, waterMaskMatrix, 255)\n",
    "waterMaskRavel = waterMasked.ravel()\n",
    "imWater = (waterMaskRavel == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating stats for predicted and truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics on test raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = np.where((waterMasked == 1) & (maskedResult == 1), 1, 0)\n",
    "tn = np.where((waterMasked == 0) & (maskedResult == 0), 1, 0)\n",
    "fp = np.where((waterMasked == 0) & (maskedResult == 1), 1, 0)\n",
    "fn = np.where((waterMasked == 1) & (maskedResult == 0), 1, 0)\n",
    "total = np.count_nonzero(waterMasked == 1) + np.count_nonzero(waterMasked == 0)\n",
    "truePositives = np.count_nonzero(tp == 1)\n",
    "trueNegatives = np.count_nonzero(tn == 1)\n",
    "falsePositives = np.count_nonzero(fp == 1)\n",
    "falseNegatives = np.count_nonzero(fn == 1)\n",
    "accuracy = (truePositives + trueNegatives) / (truePositives + trueNegatives + falsePositives + falseNegatives)\n",
    "jians = truePositives / (truePositives + trueNegatives)\n",
    "pc = truePositives / (truePositives + falsePositives)\n",
    "rc = truePositives / (truePositives + falseNegatives)\n",
    "f1 = truePositives / (truePositives + (0.5*(falsePositives + falseNegatives)))\n",
    "mcc_denom_nosqrt = (truePositives+falsePositives)*(truePositives+falseNegatives)*(trueNegatives+falsePositives)*(trueNegatives+falseNegatives)\n",
    "mcc_numerator = (truePositives*trueNegatives) - (falsePositives*falseNegatives)\n",
    "mcc = mcc_numerator/math.sqrt(mcc_denom_nosqrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count num of occurences for each class with the masked predicted result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countNoData = np.count_nonzero(maskedResult == 255)\n",
    "countLand = np.count_nonzero(maskedResult == 0)\n",
    "countWater = np.count_nonzero(maskedResult == 1)\n",
    "print('Predicted\\n Nodata occurences: {}\\n Land occurance: {}\\n Water occurances: {}'.format(countNoData, countLand, countWater))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count num of occurences for each class with the water mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countNoDataT = np.count_nonzero(waterMasked == 255)\n",
    "countLandT = np.count_nonzero(waterMasked == 0)\n",
    "countWaterT = np.count_nonzero(waterMasked == 1)\n",
    "print('Truth Vals\\n Nodata occurences: {}\\n Land occurance: {}\\n Water occurances: {}'.format(countNoDataT, countLandT, countWaterT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model metrics on raster data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Metrics of Accuracy for Raster Test Data')\n",
    "print('True Positives:  {}'.format(truePositives))\n",
    "print('True Negatives:  {}'.format(trueNegatives))\n",
    "print('False Positives: {}'.format(falsePositives))\n",
    "print('False Negatives: {}'.format(falseNegatives))\n",
    "print('Total \"good\" data: {}'.format(total))\n",
    "print('Accuracy*: {}'.format(accuracy))\n",
    "print(\"Jian's metric : {}\".format(jians))\n",
    "print('Precision: {}'.format(pc))\n",
    "print('Recall: {}'.format(rc))\n",
    "print('f1: {}'.format(f1))\n",
    "print('MCC: {}'.format(mcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output predicted raster to GeoTiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outPath = os.path.join(RASTER_OUTPUT_DIR, '{}_{}_{}_predicted_{}.tif'.format(YEAR, DAY, TILE, MODEL))\n",
    "waterMaskForDay = os.path.join(RASTER_OUTPUT_DIR, 'waterMask_{}_qa_{}.tif'.format(YEAR, DAY, TILE, MODEL))\n",
    "outPathProba = os.path.join(RASTER_OUTPUT_DIR, '{}_{}_{}_predicted_probabilities_{}.tif'.format(YEAR, DAY, TILE, MODEL))\n",
    "print(outPath)\n",
    "print(waterMaskForDay)\n",
    "print(outPathProba)\n",
    "\n",
    "ds = gdal.Open(vars_list[0], gdal.GA_ReadOnly)\n",
    "geo = ds.GetGeoTransform()\n",
    "proj = ds.GetProjection()\n",
    "ncols = ds.RasterXSize\n",
    "nrows = ds.RasterYSize\n",
    "print('Transform')\n",
    "print(geo)\n",
    "print('Projection')\n",
    "print(proj)\n",
    "print('Width')\n",
    "print(ncols)\n",
    "print('Height')\n",
    "print(nrows)\n",
    "ds = None\n",
    "\n",
    "# Output predicted binary raster masked with good-bad mask.\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "outDs = driver.Create(outPath, ncols, nrows, 1, gdal.GDT_Float32, options=['COMPRESS=LZW'])\n",
    "outDs.SetGeoTransform(geo)\n",
    "outDs.SetProjection(proj)\n",
    "outBand = outDs.GetRasterBand(1)\n",
    "outBand.WriteArray(maskedResult)\n",
    "outBand.SetNoDataValue(255)\n",
    "outDs.FlushCache()\n",
    "outDs = None\n",
    "outBand = None\n",
    "driver = None\n",
    "\n",
    "# Output water mask with good-bad masked.\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "outDs = driver.Create(waterMaskForDay, ncols, nrows, 1, gdal.GDT_Int16, options=['COMPRESS=LZW'])\n",
    "outDs.SetGeoTransform(geo)\n",
    "outDs.SetProjection(proj)\n",
    "outBand = outDs.GetRasterBand(1)\n",
    "outBand.WriteArray(waterMasked)\n",
    "outBand.SetNoDataValue(255)\n",
    "outDs.FlushCache()\n",
    "outDs = None\n",
    "outBand = None\n",
    "driver = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folium Viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import rasterio as rio\n",
    "import tempfile\n",
    "\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from pyproj import Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Uses rasterio to open a raster, get the metadata and crs\n",
    "# associated with it and get all the subdatasets in the file.\n",
    "# This is very useful for hdf files such as MODIS hdfs.\n",
    "# -----------------------------------------------------------------------------\n",
    "def print_subdatasets(filename):\n",
    "    bands_to_return = []\n",
    "    with rio.open(filename) as dataset:\n",
    "        meta_data = dataset.meta\n",
    "        crs = dataset.read_crs()\n",
    "        \n",
    "        print([name for name in dataset.subdatasets if search_term in name])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Gets a tiff that has the correct metadata for that tile, gets the metadata\n",
    "# from the source tif and copies to a destination tiff. \n",
    "# -----------------------------------------------------------------------------     \n",
    "def add_metadata_to_annual_product(filepath, model_type, year, tile):\n",
    "    metadata_pull_src = [fv for fv in glob.glob(os.path.join(filepath, \"{}-1*-{}-MOD-*.tif\".format(year, tile)))][0]\n",
    "    with rio.open(metadata_pull_src) as src:\n",
    "        src_meta = src.meta\n",
    "    dst_tiffs = [os.path.join(filepath, fn) for fn in os.listdir(filepath) if \"{0}-{1}\".format(year, tile) in os.path.basename(fn)]\n",
    "    [copy_meta(dst_tiff, src_meta, metadata_pull_src) for dst_tiff in dst_tiffs]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Given a path to a tiff with no metadata, assign the metadata given to that\n",
    "# tiff.\n",
    "# -----------------------------------------------------------------------------     \n",
    "def copy_meta(dst_path, src_meta, src_name):\n",
    "    print('Copying metadata from {} to {}'.format(src_name, dst_path))\n",
    "    with rio.open(dst_path, 'r+') as dst:\n",
    "        dst.crs = src_meta['crs']\n",
    "        dst.transform = src_meta['transform']        \n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Given a tiff file as input, open the tiff and get the transform needed to\n",
    "# reproject from the tiff's source crs to the one we want (EPSG:3857).\n",
    "# For each band in the tiff, open then reproject it into the desired crs\n",
    "# then write to a temporary file. Return the path to the temp file.\n",
    "# -----------------------------------------------------------------------------\n",
    "def reproject_to_3857(input_tiff):\n",
    "    # Set desitnation CRS\n",
    "    dst_crs = f\"EPSG:3857\"\n",
    "\n",
    "    # set out path\n",
    "    out_path_rproj = os.path.join(tempfile.gettempdir(), input_tiff.split('/')[-1].replace('.tif','-3857.tif'))\n",
    "\n",
    "    with rio.open(input_tiff) as src:\n",
    "        # get src bounds and transform\n",
    "        transform, width, height = calculate_default_transform(src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
    "        print('Transform: {}'.format(transform))\n",
    "        print('Width: {} Height: {}'.format(width, height))\n",
    "        kwargs = src.meta.copy()\n",
    "        kwargs.update({'crs': dst_crs,\n",
    "                   'transform': transform,\n",
    "                   'width': width,\n",
    "                   'height': height})\n",
    "    \n",
    "        # reproject and write to file\n",
    "        with rio.open(out_path_rproj, 'w', **kwargs) as dst:\n",
    "            for i in range(1, src.count + 1):\n",
    "                reproject(source=rio.band(src, i),\n",
    "                      destination=rio.band(dst, i),\n",
    "                      src_transform=src.transform,\n",
    "                      src_crs=src.crs,\n",
    "                      dst_transform=transform,\n",
    "                      dst_crs=dst_crs,\n",
    "                      resampling=Resampling.nearest)\n",
    "    return out_path_rproj\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# In order for folium to work properly we need to pass it the bounding box\n",
    "# of the tiff in the form of lat and lon. This is done by using rasterio.\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_bounds(tiff_3857):\n",
    "    with rio.open(tiff_3857) as src:\n",
    "        src_crs = src.crs['init'].upper()\n",
    "        min_lon, min_lat, max_lon, max_lat = src.bounds\n",
    "    bounds_orig = [[min_lat, min_lon], [max_lat, max_lon]]\n",
    "    bounds = []\n",
    "    dst_crs = 'EPSG:4326'\n",
    "    for item in bounds_orig:   \n",
    "        #converting to lat/lon\n",
    "        lat = item[0]\n",
    "        lon = item[1]\n",
    "        proj = Transformer.from_crs(int(src_crs.split(\":\")[1]), int(dst_crs.split(\":\")[1]), always_xy=True)\n",
    "        lon_n, lat_n = proj.transform(lon, lat)\n",
    "        bounds.append([lat_n, lon_n])\n",
    "    center_lon = bounds[0][1] + (bounds[1][1] - bounds[0][1])/2\n",
    "    center_lat = bounds[0][0] + (bounds[1][0] - bounds[0][0])/2\n",
    "    return {'bounds': bounds, 'center': (center_lon, center_lat)}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Use rasterio to open and read in the desired band name as a nd-array.\n",
    "# -----------------------------------------------------------------------------\n",
    "def open_and_get_band(file_name, band_num=1):\n",
    "    with rio.open(file_name) as data:\n",
    "        b = data.read(band_num)\n",
    "    return b\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Given an nd-array (band) and the bounds in lat lon of the nd-array, return\n",
    "# a folium layer. To add on the map.\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_overlay(band, meta_dict, name, opacity=1.0, show=True):\n",
    "    return folium.raster_layers.ImageOverlay(band, \n",
    "                                             bounds=meta_dict['bounds'], \n",
    "                                             name=name, \n",
    "                                             opacity=opacity, \n",
    "                                             show=show)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# We don't need to keep those temp files we made for the reprojections around.\n",
    "# -----------------------------------------------------------------------------\n",
    "def cleanup(filename):\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "    else:\n",
    "        print('No file: {} exists.'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_3857 = reproject_to_3857(outPath)\n",
    "mod44_3857 = reproject_to_3857(water_mask_path)\n",
    "qa_3857 = reproject_to_3857(qa_mask_path)\n",
    "\n",
    "mask_d = get_bounds(mask_3857)\n",
    "mod44_d = get_bounds(mod44_3857)\n",
    "qa_d = get_bounds(qa_3857)\n",
    "\n",
    "mask_b1 = open_and_get_band(mask_3857, 1)\n",
    "mask_b1 = np.where(mask_b1 == 255, 0, mask_b1)\n",
    "mod44_b1 = open_and_get_band(mod44_3857, 1)\n",
    "mod44_b1 = np.where(mod44_b1 == 255, 0, mod44_b1)\n",
    "qa_b1 = open_and_get_band(qa_3857, 1)\n",
    "\n",
    "cleanup(mask_3857)\n",
    "cleanup(mod44_3857)\n",
    "cleanup(qa_3857)\n",
    "\n",
    "zeros = np.zeros_like(mask_b1)\n",
    "mask_rgba = np.dstack((mask_b1, zeros, zeros, mask_b1))\n",
    "mod44_rgba = np.dstack((zeros, mod44_b1, zeros, mod44_b1))\n",
    "qa_rgba = np.dstack((qa_b1, qa_b1, qa_b1, qa_b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[mask_d['center'][1], mask_d['center'][0]],\n",
    "                   tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}', zoom_start = 6, attr='Google', control_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.add_child(get_overlay(mask_rgba, mask_d, '{}-{}-{} model water mask'.format(YEAR, DAY, TILE), opacity=0.8))\n",
    "m.add_child(get_overlay(mod44_rgba, mod44_d, '{} {} MOD44W mask'.format(TILE, YEAR), opacity=0.8, show=False))\n",
    "m.add_child(get_overlay(qa_rgba, qa_d, '{}-{}-{} MW QA mask'.format(YEAR, DAY, TILE), opacity=0.8, show=False))\n",
    "m.add_child(plugins.MousePosition())\n",
    "m.add_child(plugins.MeasureControl())\n",
    "m.add_child(folium.LayerControl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ILAB Kernel (Pytorch)",
   "language": "python",
   "name": "pytorch-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
