{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODIS Water Histogram-based Gradient Boosting Classification Tree\n",
    "\n",
    "Version: 0.1.0\n",
    "\n",
    "Date modified: 04.21.2022\n",
    "\n",
    "Modified by: Caleb Spradlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path   \n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, precision_score, f1_score\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, matthews_corrcoef\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "#GDAL Stuff\n",
    "from osgeo import gdalconst\n",
    "from osgeo import gdal\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURE_OUTPUT_DIR = '/adapt/nobackup/projects/ilab/scratch/cssprad1/MODIS_water/code/tmp'\n",
    "RASTER_OUTPUT_DIR = '/adapt/nobackup/projects/ilab/scratch/cssprad1/MODIS_water/code/tmp'\n",
    "MODEL_OUTPUT_DIR = '/adapt/nobackup/projects/ilab/scratch/cssprad1/MODIS_water/models/'\n",
    "\n",
    "training_data_basepath = '/adapt/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/training_data/v4.0.0'\n",
    "\n",
    "qaMaskPath = '/adapt/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/qa'\n",
    "waterMaskPath = '/adapt/nobackup/people/mcarrol2/MODIS_water/v5_outputs'\n",
    "\n",
    "test_data_basepath = '/adapt/nobackup/projects/ilab/data/MODIS/MODIS_WATER_ML/test_data/'\n",
    "\n",
    "GPU = True\n",
    "TILE = 'Golden'\n",
    "MODEL = 'HGBM'\n",
    "TEST_RATIO = 0.2\n",
    "RANDOM_STATE = 42\n",
    "LABEL_NAME = 'water'\n",
    "DATA_TYPE = np.int16\n",
    "# Columns that are offset, years, julian days, etc (always need to be dropped).\n",
    "offsets_indexes = ['x_offset', 'y_offset', 'year', 'julian_day', 'tileID']\n",
    "# Columns that the user wants to drop for training purposes. \n",
    "colsToDrop = []#'sur_refl_b03_1','sur_refl_b04_1','sur_refl_b05_1','ndwi1','ndwi2']\n",
    "colsToDropTraining = colsToDrop.copy()\n",
    "colsToDropTraining.extend(offsets_indexes)\n",
    "v_names = ['sur_refl_b01_1','sur_refl_b02_1','sur_refl_b03_1',\n",
    "           'sur_refl_b04_1','sur_refl_b05_1','sur_refl_b06_1',\n",
    "           'sur_refl_b07_1','ndvi','ndwi1','ndwi2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsToDrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsToDropTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fpath, colsToDrop, yCol='water', testSize=0.2, randomState=42, \n",
    "              dataType=np.float32, cpu=True, splitXY=False, trainTestSplit=False,\n",
    "             applyLog=False, imbalance=False, frac=0.1, land=False, multi=False, \n",
    "              multisample=1000000):\n",
    "    \"\"\"\n",
    "    Simple helper function for loading data to be used by models\n",
    "    :param fpath: Path to the data to be ingested.\n",
    "    :param dataType: Data type to convert ingested data to.\n",
    "    :param colsToDrop: Columns which are not necessary, from which to drop.\n",
    "    :param testSize: Ration to\n",
    "    \"\"\"\n",
    "    if multi:\n",
    "        all_dfs = [pd.read_csv(path_) for path_ in fpath]\n",
    "        df = pd.concat(all_dfs).sample(n=multisample, random_state=randomState)\n",
    "        print('DF length: {}'.format(len(df.index)))\n",
    "    else:   \n",
    "        df = pd.read_parquet(fpath) if '.parquet' in fpath else pd.read_csv(fpath)\n",
    "    df = df[df['sur_refl_b01_1'] + df['sur_refl_b02_1'] != 0]\n",
    "    df = df[df['sur_refl_b07_1'] + df['sur_refl_b02_1'] != 0]\n",
    "    df = df[df['sur_refl_b06_1'] + df['sur_refl_b02_1'] != 0]\n",
    "    df = df.drop(columns=colsToDrop)\n",
    "    cleanedDF = df[~df.isin([np.NaN, np.inf, -np.inf]).any(1)].dropna(axis=0).astype(dataType)\n",
    "    if applyLog:\n",
    "        for col in cleanedDF.drop([yCol], axis=1).columns:\n",
    "            print('Applying log1p func to {}'.format(col))\n",
    "            cleanedDF[col] = np.log1p(cleanedDF[col])\n",
    "        cleanedDF = cleanedDF[~cleanedDF.isin([np.NaN, np.inf, -np.inf]).any(1)].dropna(axis=0)\n",
    "    df = None\n",
    "    if imbalance:\n",
    "        if land:\n",
    "            print('Imbalancing data, sampling {} from water'.format(frac))\n",
    "        else:\n",
    "            print('Imbalancing data, sampling {} from land'.format(frac))\n",
    "        groupedDF = cleanedDF.groupby('water')\n",
    "        dfs = [groupedDF.get_group(y) for y in groupedDF.groups]\n",
    "        sampledDF = dfs[1].sample(frac=frac)if land else dfs[0].sample(frac=frac)\n",
    "        concatDF = sampledDF.append(dfs[0]) if land else sampledDF.append(dfs[1])\n",
    "        concatDF = concatDF.sample(frac=1)\n",
    "        concatDF = concatDF.reset_index()\n",
    "        cleanedDF = concatDF.drop(columns=['index'])\n",
    "    if not splitXY:\n",
    "        return cleanedDF\n",
    "    X = cleanedDF.drop([yCol], axis=1).astype(dataType)\n",
    "    y = cleanedDF[yCol].astype(dataType)\n",
    "    if trainTestSplit:\n",
    "        return train_test_split(X, y, test_size=TEST_RATIO)\n",
    "    else:\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "- Read in to cuDF or pandas Dataframe\n",
    "- Drop unnecessary columns\n",
    "- Clean data\n",
    "- Split into Xs and Ys\n",
    "- Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_string = os.path.join(training_data_basepath,'MOD*{}*.parquet.gzip'.format(TILE))\n",
    "data_paths = [fv for fv in glob.glob(glob_string)]\n",
    "data_path = data_paths[0]\n",
    "pprint(data_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_data(fpath=data_path,\n",
    "                                             colsToDrop=colsToDropTraining,\n",
    "                                             dataType=DATA_TYPE,\n",
    "                                             cpu=True,\n",
    "                                             splitXY=True,\n",
    "                                             imbalance=False,\n",
    "                                             land=True,\n",
    "                                             frac=0.75,\n",
    "                                             trainTestSplit=True,\n",
    "                                             multi=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [print(column) for column in X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with the best hyperparamers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'categorical_features': None,\n",
    " 'early_stopping': 'auto',\n",
    " 'l2_regularization': 0.0,\n",
    " 'learning_rate': 0.1,\n",
    " 'loss': 'auto',\n",
    " 'max_bins': 255,\n",
    " 'max_depth': None,\n",
    " 'max_iter': 100,\n",
    " 'max_leaf_nodes': 31,\n",
    " 'min_samples_leaf': 20,\n",
    " 'monotonic_cst': None,\n",
    " 'n_iter_no_change': 10,\n",
    " 'random_state': None,\n",
    " 'scoring': 'loss',\n",
    " 'tol': 1e-07,\n",
    " 'validation_fraction': 0.1,\n",
    " 'verbose': 0,\n",
    " 'warm_start': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = HistGradientBoostingClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing and training/testing data validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = classifier.score(X_test, y_test)\n",
    "score = round(score, 3)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = classifier.predict(X_train)\n",
    "test_predictions = classifier.predict(X_test)\n",
    "prediction_probs = classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = test_predictions.astype(np.int16)\n",
    "y_test_int = y_test.astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Performance')\n",
    "print('-------------------------------------------------------')\n",
    "print(classification_report(y_test, test_predictions))\n",
    "# From docs: tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_int, test_predictions).ravel()\n",
    "recall = (tp / (tp + fp))\n",
    "print('Test Recall')\n",
    "print('-------------------------------------------------------')\n",
    "print(recall)\n",
    "print('\\nTest Matthews Correlation Coefficient (MCC)')\n",
    "print('-------------------------------------------------------')\n",
    "mcc = matthews_corrcoef(y_test_int, test_predictions)\n",
    "print(mcc)\n",
    "print('\\nConfusion Matrix')\n",
    "print('-------------------------------------------------------')\n",
    "print('TP: {:9} FN: {:9}'.format(tp, fn))\n",
    "print('FP: {:9} TN: {:9}'.format(fp, tn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = classifier\n",
    "\n",
    "probs = clf.predict_proba(X_test)\n",
    "preds = probs[:, 1]\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'red', label = 'ROC AUC score = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'b--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutaion importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_importance_results = permutation_importance(classifier,\n",
    "                                                        X=X_test,\n",
    "                                                        y=y_test,\n",
    "                                                        n_repeats=5,\n",
    "                                                        random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png_save_path = 'mw_{}_{}_{}_{}_{}_permutation_importance.png'.format(\n",
    "    TILE,\n",
    "    score,\n",
    "    MODEL,\n",
    "    params['max_iter'],\n",
    "    datetime.datetime.now().strftime('%Y_%m_%d_%H_%M'))\n",
    "\n",
    "png_save_path = os.path.join(FIGURE_OUTPUT_DIR, png_save_path)\n",
    "\n",
    "sorted_idx = permutation_importance_results.importances_mean.argsort()\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.barh(X_test.columns[sorted_idx], permutation_importance_results.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "plt.savefig(png_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, X_test, y_train, y_test, test_predictions, train_predictions, y_test_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'mw_{}_{}_{}_{}_4.0.0_tuned_{}.sav'.format(TILE,\n",
    "                                                              score,\n",
    "                                                              MODEL,\n",
    "                                                              params['max_iter'],\n",
    "                                                              'cpu',\n",
    "                                                              datetime.datetime.now().strftime('%Y_%m_%d_%H_%M'))\n",
    "model_save_path = os.path.join(MODEL_OUTPUT_DIR, model_save_path)\n",
    "print('Saving model to: {}'.format(model_save_path))\n",
    "print(classifier)\n",
    "joblib.dump(classifier, model_save_path, compress=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing: raster testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE = 'h11v10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = 201\n",
    "YEAR = 2006\n",
    "PATH = os.path.join(test_data_basepath, '{}/'.format(TILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_list_gq = [fn for fn in glob.glob(os.path.join(PATH, '*A{}{:03}*.tif'.format(YEAR, DAY)))\n",
    "            if 'sur_refl' in fn and 'GQ' in fn]\n",
    "vars_list_gq.sort()\n",
    "\n",
    "vars_list_ga = [fn for fn in glob.glob(os.path.join(PATH, '*A{}{:03}*.tif'.format(YEAR, DAY)))\n",
    "            if 'sur_refl' in fn and 'GQ' not in fn]\n",
    "vars_list_ga.sort()\n",
    "\n",
    "vars_list = vars_list_gq\n",
    "vars_list.extend(vars_list_ga)\n",
    "\n",
    "vars_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dimensions of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vrt_opts = gdal.BuildVRTOptions(separate=True)\n",
    "dd = gdal.BuildVRT('tmp.vrt', vars_list, options=vrt_opts)\n",
    "nrows, ncols = dd.RasterYSize, dd.RasterXSize\n",
    "dd = None\n",
    "if os.path.exists('tmp.vrt'):\n",
    "    os.remove('tmp.vrt') \n",
    "nrows, ncols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data \n",
    "We don't need to slice because we have more than enough GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readRasterToArray(vars_list):\n",
    "    vrt_options = gdal.BuildVRTOptions(separate=True)\n",
    "    dd = gdal.BuildVRT('tmp.vrt', vars_list, options=vrt_options)\n",
    "    nrows, ncols = dd.RasterYSize, dd.RasterXSize\n",
    "    newshp = (ncols*nrows, dd.RasterCount+3)\n",
    "    img = np.empty(newshp, dtype=np.int16)\n",
    "    for b in range(len(vars_list)):\n",
    "        img[:, b] = dd.GetRasterBand(b+1).ReadAsArray().astype(np.int16).ravel()\n",
    "    dd = None\n",
    "    img[:, len(vars_list)] = ((img[:, 1] - img[:, 0]) / (img[:, 1] + img[:, 0])) * 10000\n",
    "    img[:, len(vars_list)+1] = ((img[:, 1] - img[:, 5]) / (img[:, 1] + img[:, 5])) * 10000\n",
    "    img[:, len(vars_list)+2] = ((img[:, 1] - img[:, 6]) / (img[:, 1] + img[:, 6])) * 10000\n",
    "    if os.path.exists('tmp.vrt'):\n",
    "        os.remove('tmp.vrt')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = readRasterToArray(vars_list)\n",
    "print('Raster as ndarray')\n",
    "print(im)\n",
    "print('{} MB size'.format((im.size * im.itemsize) / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRaster(img_chunk, colsToDrop=None):\n",
    "    \"\"\"\n",
    "    Function given a raster in the form of a nxn matrix, will\n",
    "    convert the matrix to a GPU/CPU-bound data frame then perform \n",
    "    predictions given the loaded model.\n",
    "    \n",
    "    Return the prediction matrix, the prediction probabilities\n",
    "    for each and the dataframe converted to host.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(img_chunk, columns=v_names, dtype=np.int16)\n",
    "    df = df.drop(columns=colsToDrop) if colsToDrop else df\n",
    "    print('Making predictions from raster')\n",
    "    predictions = classifier.predict(df).astype(np.int16)\n",
    "    predictionsProbs = classifier.predict_proba(df).astype(np.float32)\n",
    "    return predictions, predictionsProbs, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictedRaster, predictedProbaRaster, df = predictRaster(im, colsToDrop=colsToDrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp = (4800, 4800)\n",
    "left = list()\n",
    "right = list()\n",
    "for i, subarr in enumerate(predictedProbaRaster):\n",
    "    left.append(subarr[0])\n",
    "    right.append(subarr[1])\n",
    "leftArr = np.asarray(left)\n",
    "rightArr = np.asarray(right)\n",
    "probaLand = leftArr.reshape(shp)\n",
    "probaWater = rightArr.reshape(shp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raster as a DataFrame: description and histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape the unravelled matrix back to the 4800x4800 raster shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.asarray(predictedRaster)\n",
    "reshp = matrix.reshape(shp)\n",
    "reshp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the QA Mask and the Water Mask for the h09v05 TILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_list = [fn for fn in glob.glob(os.path.join(qaMaskPath, '{}/*A{}{:03}.{}*.tif'.format(TILE, YEAR, DAY, TILE)))]\n",
    "try:\n",
    "    qa_mask = qa_list[0]\n",
    "    print(qa_mask)\n",
    "    ds = gdal.Open(qa_mask, gdal.GA_ReadOnly)\n",
    "    qaMaskMatrix = ds.GetRasterBand(1).ReadAsArray().astype(np.int16)\n",
    "except Exception:\n",
    "    path = os.path.join(qaMaskPath, '{}/*A{}{:03}.{}*.tif'.format(TILE, YEAR, DAY, TILE))\n",
    "    print('Could not find QA mask at {}'.format(path))\n",
    "    qaMaskMatrix = np.zeros((4800, 4800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_list = [fn for fn in glob.glob(os.path.join(waterMaskPath, '{}'.format(YEAR), '*{}*{}_v5.tif'.format(TILE, YEAR)))]\n",
    "print(os.path.join(waterMaskPath, '{}'.format(YEAR), '*{}*.tif'.format(TILE)))\n",
    "water_mask_path = water_list[0]\n",
    "print(water_mask_path)\n",
    "water_mask = gdal.Open(water_mask_path, gdal.GA_ReadOnly)\n",
    "waterMaskMatrix = water_mask.GetRasterBand(1).ReadAsArray().astype(np.int16)\n",
    "waterMask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterMaskMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask out results if QA Mask says pixel is \"bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskedResult = np.where(qaMaskMatrix == 0, reshp, -9999)\n",
    "maskedResultProba = np.where(qaMaskMatrix == 0, probaWater, -9999)\n",
    "waterMasked = np.where(qaMaskMatrix == 0, waterMaskMatrix, -9999)\n",
    "waterMaskRavel = waterMasked.ravel()\n",
    "imWater = (waterMaskRavel == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating stats for predicted and truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics on test raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = np.where((waterMasked == 1) & (maskedResult == 1), 1, 0)\n",
    "tn = np.where((waterMasked == 0) & (maskedResult == 0), 1, 0)\n",
    "fp = np.where((waterMasked == 0) & (maskedResult == 1), 1, 0)\n",
    "fn = np.where((waterMasked == 1) & (maskedResult == 0), 1, 0)\n",
    "total = np.count_nonzero(waterMasked == 1) + np.count_nonzero(waterMasked == 0)\n",
    "truePositives = np.count_nonzero(tp == 1)\n",
    "trueNegatives = np.count_nonzero(tn == 1)\n",
    "falsePositives = np.count_nonzero(fp == 1)\n",
    "falseNegatives = np.count_nonzero(fn == 1)\n",
    "accuracy = (truePositives + trueNegatives) / (truePositives + trueNegatives + falsePositives + falseNegatives)\n",
    "jians = truePositives / (truePositives + trueNegatives)\n",
    "pc = truePositives / (truePositives + falsePositives)\n",
    "rc = truePositives / (truePositives + falseNegatives)\n",
    "f1 = truePositives / (truePositives + (0.5*(falsePositives + falseNegatives)))\n",
    "mcc_denom_nosqrt = (truePositives+falsePositives)*(truePositives+falseNegatives)*(trueNegatives+falsePositives)*(trueNegatives+falseNegatives)\n",
    "mcc_numerator = (truePositives*trueNegatives) - (falsePositives*falseNegatives)\n",
    "mcc = mcc_numerator/math.sqrt(mcc_denom_nosqrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count num of occurences for each class with the masked predicted result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countNoData = np.count_nonzero(maskedResult == -9999)\n",
    "countLand = np.count_nonzero(maskedResult == 0)\n",
    "countWater = np.count_nonzero(maskedResult == 1)\n",
    "print('Predicted\\n Nodata occurences: {}\\n Land occurance: {}\\n Water occurances: {}'.format(countNoData, countLand, countWater))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count num of occurences for each class with the water mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countNoDataT = np.count_nonzero(waterMasked == -9999)\n",
    "countLandT = np.count_nonzero(waterMasked == 0)\n",
    "countWaterT = np.count_nonzero(waterMasked == 1)\n",
    "print('Min. extent water mask vals\\n Nodata occurences: {}\\n Land occurance: {}\\n Water occurances: {}'.format(countNoDataT, countLandT, countWaterT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model metrics on raster data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Metrics of Accuracy for Raster Test Data')\n",
    "print('True Positives:  {}'.format(truePositives))\n",
    "print('True Negatives:  {}'.format(trueNegatives))\n",
    "print('False Positives: {}'.format(falsePositives))\n",
    "print('False Negatives: {}'.format(falseNegatives))\n",
    "print('Total \"good\" data: {}'.format(total))\n",
    "print('Accuracy*: {}'.format(accuracy))\n",
    "print(\"Jian's metric : {}\".format(jians))\n",
    "print('Precision: {}'.format(pc))\n",
    "print('Recall: {}'.format(rc))\n",
    "print('f1: {}'.format(f1))\n",
    "print('MCC: {}'.format(mcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output predicted raster to GeoTiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outPath = os.path.join(RASTER_OUTPUT_DIR, '{}_{}_{}_predicted_{}.tif'.format(YEAR, DAY, TILE, MODEL))\n",
    "waterMaskForDay = os.path.join(RASTER_OUTPUT_DIR, 'waterMask_{}_qa_{}.tif'.format(YEAR, DAY, TILE, MODEL))\n",
    "outPathProba = os.path.join(RASTER_OUTPUT_DIR, '{}_{}_{}_predicted_probabilities_{}.tif'.format(YEAR, DAY, TILE, MODEL))\n",
    "print(outPath)\n",
    "print(waterMaskForDay)\n",
    "print(outPathProba)\n",
    "\n",
    "ds = gdal.Open(vars_list[0], gdal.GA_ReadOnly)\n",
    "geo = ds.GetGeoTransform()\n",
    "proj = ds.GetProjection()\n",
    "ncols = ds.RasterXSize\n",
    "nrows = ds.RasterYSize\n",
    "print('Transform')\n",
    "print(geo)\n",
    "print('Projection')\n",
    "print(proj)\n",
    "print('Width')\n",
    "print(ncols)\n",
    "print('Height')\n",
    "print(nrows)\n",
    "ds = None\n",
    "\n",
    "# Output predicted binary raster masked with good-bad mask.\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "outDs = driver.Create(outPath, ncols, nrows, 1, gdal.GDT_Int16, options=['COMPRESS=LZW'])\n",
    "outDs.SetGeoTransform(geo)\n",
    "outDs.SetProjection(proj)\n",
    "outBand = outDs.GetRasterBand(1)\n",
    "outBand.WriteArray(maskedResult)\n",
    "outBand.SetNoDataValue(-9999)\n",
    "outDs.FlushCache()\n",
    "outDs = None\n",
    "outBand = None\n",
    "driver = None\n",
    "\n",
    "# Output water mask with good-bad masked.\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "outDs = driver.Create(waterMaskForDay, ncols, nrows, 1, gdal.GDT_Int16, options=['COMPRESS=LZW'])\n",
    "outDs.SetGeoTransform(geo)\n",
    "outDs.SetProjection(proj)\n",
    "outBand = outDs.GetRasterBand(1)\n",
    "outBand.WriteArray(waterMasked)\n",
    "outBand.SetNoDataValue(-9999)\n",
    "outDs.FlushCache()\n",
    "outDs = None\n",
    "outBand = None\n",
    "driver = None\n",
    "\n",
    "# Output probabilies raster masked by good-bad mask.\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "outDs = driver.Create(outPathProba, ncols, nrows, 1, gdal.GDT_Float32, options=['COMPRESS=LZW'])\n",
    "outDs.SetGeoTransform(geo)\n",
    "outDs.SetProjection(proj)\n",
    "outBand = outDs.GetRasterBand(1)\n",
    "outBand.WriteArray(maskedResultProba)\n",
    "outBand.SetNoDataValue(-9999)\n",
    "outDs.FlushCache()\n",
    "outDs = None\n",
    "outBand = None\n",
    "driver = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folium Viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import rasterio as rio\n",
    "import tempfile\n",
    "\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from pyproj import Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Uses rasterio to open a raster, get the metadata and crs\n",
    "# associated with it and get all the subdatasets in the file.\n",
    "# This is very useful for hdf files such as MODIS hdfs.\n",
    "# -----------------------------------------------------------------------------\n",
    "def print_subdatasets(filename):\n",
    "    bands_to_return = []\n",
    "    with rio.open(filename) as dataset:\n",
    "        meta_data = dataset.meta\n",
    "        crs = dataset.read_crs()\n",
    "        \n",
    "        print([name for name in dataset.subdatasets if search_term in name])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Gets a tiff that has the correct metadata for that tile, gets the metadata\n",
    "# from the source tif and copies to a destination tiff. \n",
    "# -----------------------------------------------------------------------------     \n",
    "def add_metadata_to_annual_product(filepath, model_type, year, tile):\n",
    "    metadata_pull_src = [fv for fv in glob.glob(os.path.join(filepath, \"{}-1*-{}-MOD-*.tif\".format(year, tile)))][0]\n",
    "    with rio.open(metadata_pull_src) as src:\n",
    "        src_meta = src.meta\n",
    "    dst_tiffs = [os.path.join(filepath, fn) for fn in os.listdir(filepath) if \"{0}-{1}\".format(year, tile) in os.path.basename(fn)]\n",
    "    [copy_meta(dst_tiff, src_meta, metadata_pull_src) for dst_tiff in dst_tiffs]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Given a path to a tiff with no metadata, assign the metadata given to that\n",
    "# tiff.\n",
    "# -----------------------------------------------------------------------------     \n",
    "def copy_meta(dst_path, src_meta, src_name):\n",
    "    print('Copying metadata from {} to {}'.format(src_name, dst_path))\n",
    "    with rio.open(dst_path, 'r+') as dst:\n",
    "        dst.crs = src_meta['crs']\n",
    "        dst.transform = src_meta['transform']        \n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Given a tiff file as input, open the tiff and get the transform needed to\n",
    "# reproject from the tiff's source crs to the one we want (EPSG:3857).\n",
    "# For each band in the tiff, open then reproject it into the desired crs\n",
    "# then write to a temporary file. Return the path to the temp file.\n",
    "# -----------------------------------------------------------------------------\n",
    "def reproject_to_3857(input_tiff):\n",
    "    # Set desitnation CRS\n",
    "    dst_crs = f\"EPSG:3857\"\n",
    "\n",
    "    # set out path\n",
    "    out_path_rproj = os.path.join(tempfile.gettempdir(), input_tiff.split('/')[-1].replace('.tif','-3857.tif'))\n",
    "\n",
    "    with rio.open(input_tiff) as src:\n",
    "        # get src bounds and transform\n",
    "        transform, width, height = calculate_default_transform(src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
    "        print('Transform: {}'.format(transform))\n",
    "        print('Width: {} Height: {}'.format(width, height))\n",
    "        kwargs = src.meta.copy()\n",
    "        kwargs.update({'crs': dst_crs,\n",
    "                   'transform': transform,\n",
    "                   'width': width,\n",
    "                   'height': height})\n",
    "    \n",
    "        # reproject and write to file\n",
    "        with rio.open(out_path_rproj, 'w', **kwargs) as dst:\n",
    "            for i in range(1, src.count + 1):\n",
    "                reproject(source=rio.band(src, i),\n",
    "                      destination=rio.band(dst, i),\n",
    "                      src_transform=src.transform,\n",
    "                      src_crs=src.crs,\n",
    "                      dst_transform=transform,\n",
    "                      dst_crs=dst_crs,\n",
    "                      resampling=Resampling.nearest)\n",
    "    return out_path_rproj\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# In order for folium to work properly we need to pass it the bounding box\n",
    "# of the tiff in the form of lat and lon. This is done by using rasterio.\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_bounds(tiff_3857):\n",
    "    with rio.open(tiff_3857) as src:\n",
    "        src_crs = src.crs['init'].upper()\n",
    "        min_lon, min_lat, max_lon, max_lat = src.bounds\n",
    "    bounds_orig = [[min_lat, min_lon], [max_lat, max_lon]]\n",
    "    bounds = []\n",
    "    dst_crs = 'EPSG:4326'\n",
    "    for item in bounds_orig:   \n",
    "        #converting to lat/lon\n",
    "        lat = item[0]\n",
    "        lon = item[1]\n",
    "        proj = Transformer.from_crs(int(src_crs.split(\":\")[1]), int(dst_crs.split(\":\")[1]), always_xy=True)\n",
    "        lon_n, lat_n = proj.transform(lon, lat)\n",
    "        bounds.append([lat_n, lon_n])\n",
    "    center_lon = bounds[0][1] + (bounds[1][1] - bounds[0][1])/2\n",
    "    center_lat = bounds[0][0] + (bounds[1][0] - bounds[0][0])/2\n",
    "    return {'bounds': bounds, 'center': (center_lon, center_lat)}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Use rasterio to open and read in the desired band name as a nd-array.\n",
    "# -----------------------------------------------------------------------------\n",
    "def open_and_get_band(file_name, band_num=1):\n",
    "    with rio.open(file_name) as data:\n",
    "        b = data.read(band_num)\n",
    "    return b\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Given an nd-array (band) and the bounds in lat lon of the nd-array, return\n",
    "# a folium layer. To add on the map.\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_overlay(band, meta_dict, name, opacity=1.0, show=True):\n",
    "    return folium.raster_layers.ImageOverlay(band, \n",
    "                                             bounds=meta_dict['bounds'], \n",
    "                                             name=name, \n",
    "                                             opacity=opacity, \n",
    "                                             show=show)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# We don't need to keep those temp files we made for the reprojections around.\n",
    "# -----------------------------------------------------------------------------\n",
    "def cleanup(filename):\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "    else:\n",
    "        print('No file: {} exists.'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_3857 = reproject_to_3857(outPath)\n",
    "probW_3857 = reproject_to_3857(outPathProba)\n",
    "mod44_3857 = reproject_to_3857(water_mask_path)\n",
    "qa_3857 = reproject_to_3857(qa_mask)\n",
    "\n",
    "mask_d = get_bounds(mask_3857)\n",
    "probw_d = get_bounds(probW_3857)\n",
    "mod44_d = get_bounds(mod44_3857)\n",
    "qa_d = get_bounds(qa_3857)\n",
    "\n",
    "mask_b1 = open_and_get_band(mask_3857, 1)\n",
    "probw_b1 = open_and_get_band(probW_3857, 1)\n",
    "mod44_b1 = open_and_get_band(mod44_3857, 1)\n",
    "mod44_b1 = np.where(mod44_b1 == 255, 0, mod44_b1)\n",
    "qa_b1 = open_and_get_band(qa_3857, 1)\n",
    "\n",
    "cleanup(mask_3857)\n",
    "cleanup(probW_3857)\n",
    "cleanup(mod44_3857)\n",
    "cleanup(qa_3857)\n",
    "\n",
    "zeros = np.zeros_like(mask_b1)\n",
    "mask_rgba = np.dstack((mask_b1, zeros, zeros, mask_b1))\n",
    "probw_rgb = np.dstack((zeros, zeros, probw_b1))\n",
    "mod44_rgba = np.dstack((zeros, mod44_b1, zeros, mod44_b1))\n",
    "qa_rgba = np.dstack((qa_b1, qa_b1, qa_b1, qa_b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[mask_d['center'][1], mask_d['center'][0]],\n",
    "                   tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}', zoom_start = 6, attr='Google', control_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.add_child(get_overlay(mask_rgba, mask_d, '{}-{}-{} model water mask'.format(YEAR, DAY, TILE), opacity=0.8))\n",
    "m.add_child(get_overlay(probw_rgb, probw_d, '{}-{}-{} model proba water'.format(YEAR, DAY, TILE), opacity=0.35, show=False))\n",
    "m.add_child(get_overlay(mod44_rgba, mod44_d, '{} {} MOD44W mask'.format(TILE, YEAR), opacity=0.8, show=False))\n",
    "m.add_child(get_overlay(qa_rgba, qa_d, '{}-{}-{} MW QA mask'.format(YEAR, DAY, TILE), opacity=0.8, show=False))\n",
    "m.add_child(plugins.MousePosition())\n",
    "m.add_child(folium.LayerControl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ilab]",
   "language": "python",
   "name": "conda-env-ilab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
